{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "trying-eleven",
   "metadata": {},
   "source": [
    "### イメージを読み込む"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "executive-valuable",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy  as np\n",
    "import os\n",
    "import glob \n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "controlled-furniture",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sprint18_image\n",
    "\n",
    "path = './sprint18_image/training/' \n",
    "flist = os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "latin-cursor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m.\u001b[m\u001b[m          .DS_Store  cat.02.jpg cat.04.jpg dog.01.jpg dog.03.jpg dog.05.jpg\r\n",
      "\u001b[34m..\u001b[m\u001b[m         cat.01.jpg cat.03.jpg cat.05.jpg dog.02.jpg dog.04.jpg dog.05.xml\r\n"
     ]
    }
   ],
   "source": [
    "!ls -a .//sprint18_image/training/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "conscious-accountability",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.DS_Store',\n",
       " 'cat.01.jpg',\n",
       " 'cat.02.jpg',\n",
       " 'cat.03.jpg',\n",
       " 'cat.04.jpg',\n",
       " 'cat.05.jpg',\n",
       " 'dog.01.jpg',\n",
       " 'dog.02.jpg',\n",
       " 'dog.03.jpg',\n",
       " 'dog.04.jpg',\n",
       " 'dog.05.jpg',\n",
       " 'dog.05.xml']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "bound-america",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cat.01.jpg',\n",
       " 'cat.02.jpg',\n",
       " 'cat.03.jpg',\n",
       " 'cat.04.jpg',\n",
       " 'cat.05.jpg',\n",
       " 'dog.01.jpg',\n",
       " 'dog.02.jpg',\n",
       " 'dog.03.jpg',\n",
       " 'dog.04.jpg',\n",
       " 'dog.05.jpg',\n",
       " 'dog.05.xml']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 不可視ファイルの.DS_Storeファイルを除いて読み込む\n",
    "\n",
    "'''\n",
    "余談\n",
    "\n",
    ".DS_Storeファイルとは？ 開けるの？\n",
    "\n",
    "https://miloserdov.org/?p=3867\n",
    "\n",
    "'''\n",
    "\n",
    "flist_ignore = [name for name in os.listdir(path) if not name.startswith('.')]\n",
    "flist_ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "nearby-criminal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./sprint18_image/training/cat.01.jpg',\n",
       " './sprint18_image/training/cat.02.jpg',\n",
       " './sprint18_image/training/cat.03.jpg',\n",
       " './sprint18_image/training/cat.04.jpg',\n",
       " './sprint18_image/training/cat.05.jpg',\n",
       " './sprint18_image/training/dog.01.jpg',\n",
       " './sprint18_image/training/dog.02.jpg',\n",
       " './sprint18_image/training/dog.03.jpg',\n",
       " './sprint18_image/training/dog.04.jpg',\n",
       " './sprint18_image/training/dog.05.jpg']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_list = glob.glob(path + '/*' + \".jpg\")\n",
    "img_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "framed-prerequisite",
   "metadata": {},
   "source": [
    "### イメージのロード、配列化、リサイズ、データセット作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "vietnamese-causing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.resizeはだめ、ぜったい\n",
    "\n",
    "dog_img_array = np.empty((0,224,224,3))\n",
    "cat_img_array = np.empty((0,224,224,3))\n",
    "\n",
    "for img in img_list:\n",
    "    \n",
    "    # ファイル名に'dog'が含まれるイメージ\n",
    "    if re.search('dog', img):\n",
    "        \n",
    "        dog_img_ = Image.open(img)\n",
    "        \n",
    "        # サイズを揃える\n",
    "        dog_img_ = dog_img_.resize((224, 224))\n",
    "        \n",
    "        # PIL.Image.Imageからnumpy配列へ\n",
    "        dog_img = np.array(dog_img_)\n",
    "        \n",
    "        # 正規化\n",
    "        dog_img = dog_img / 255.\n",
    "        \n",
    "        # axisの追加\n",
    "        dog_img = dog_img.reshape((1,224,224,3))\n",
    "        \n",
    "        dog_img_array = np.concatenate([dog_img_array, dog_img], axis = 0)\n",
    "        \n",
    "        dog_img_.close()\n",
    "    \n",
    "    # ファイル名に'cat'が含まれるイメージ\n",
    "    if re.search('cat', img):\n",
    "        \n",
    "        cat_img_ = Image.open(img)\n",
    "        \n",
    "        cat_img_ = cat_img_.resize((224, 224))\n",
    "        \n",
    "        cat_img = np.array(cat_img_)\n",
    "        \n",
    "        cat_img = cat_img / 255.\n",
    "        \n",
    "        cat_img = cat_img.reshape((1,224,224,3))\n",
    "        \n",
    "        cat_img_array = np.concatenate([cat_img_array, cat_img], axis = 0)\n",
    "        \n",
    "        cat_img_.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "reliable-peripheral",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog_image:(5, 224, 224, 3)  cat_image:(5, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "print('dog_image:{}  cat_image:{}'.format(dog_img_array.shape, cat_img_array.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analyzed-companion",
   "metadata": {},
   "source": [
    "### イメージの出力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "young-output",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "データ型: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[0.27058824, 0.1254902 , 0.09803922],\n",
       "        [0.2745098 , 0.1372549 , 0.09803922],\n",
       "        [0.27058824, 0.13333333, 0.09411765],\n",
       "        ...,\n",
       "        [0.42352941, 0.39607843, 0.35294118],\n",
       "        [0.40784314, 0.39215686, 0.34509804],\n",
       "        [0.4       , 0.39215686, 0.3372549 ]],\n",
       "\n",
       "       [[0.30980392, 0.14509804, 0.11372549],\n",
       "        [0.33333333, 0.19607843, 0.12941176],\n",
       "        [0.30196078, 0.17254902, 0.12156863],\n",
       "        ...,\n",
       "        [0.41960784, 0.39607843, 0.36078431],\n",
       "        [0.41568627, 0.39607843, 0.35686275],\n",
       "        [0.41176471, 0.39607843, 0.35294118]],\n",
       "\n",
       "       [[0.29803922, 0.14901961, 0.10980392],\n",
       "        [0.34117647, 0.18431373, 0.1254902 ],\n",
       "        [0.31372549, 0.17647059, 0.11372549],\n",
       "        ...,\n",
       "        [0.41960784, 0.40392157, 0.36078431],\n",
       "        [0.41568627, 0.40392157, 0.35686275],\n",
       "        [0.41176471, 0.4       , 0.35686275]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.4       , 0.16078431, 0.10196078],\n",
       "        [0.41176471, 0.18039216, 0.10980392],\n",
       "        [0.38823529, 0.15686275, 0.10980392],\n",
       "        ...,\n",
       "        [0.81568627, 0.6745098 , 0.57647059],\n",
       "        [0.71764706, 0.59215686, 0.4745098 ],\n",
       "        [0.62745098, 0.50196078, 0.37254902]],\n",
       "\n",
       "       [[0.40784314, 0.16078431, 0.10196078],\n",
       "        [0.42352941, 0.17647059, 0.10588235],\n",
       "        [0.44705882, 0.21960784, 0.1372549 ],\n",
       "        ...,\n",
       "        [0.90588235, 0.76078431, 0.66666667],\n",
       "        [0.88627451, 0.74117647, 0.65490196],\n",
       "        [0.83529412, 0.70196078, 0.6       ]],\n",
       "\n",
       "       [[0.41568627, 0.16470588, 0.11372549],\n",
       "        [0.44705882, 0.20784314, 0.1372549 ],\n",
       "        [0.43529412, 0.20392157, 0.1372549 ],\n",
       "        ...,\n",
       "        [0.9254902 , 0.77647059, 0.66666667],\n",
       "        [0.89411765, 0.7372549 , 0.62745098],\n",
       "        [0.8745098 , 0.71764706, 0.61176471]]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 配列のまま出力\n",
    "\n",
    "print('データ型:', cat_img_array[3].dtype)\n",
    "\n",
    "cat_img_array[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "interested-university",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配列を画像として出力\n",
    "\n",
    "#plt.rcParams['figure.figsize'] = (5.0, 5.0)\n",
    "\n",
    "#plt.imshow(cat_img_array[1])\n",
    "\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "homeless-third",
   "metadata": {},
   "outputs": [],
   "source": [
    "#'''\n",
    "\n",
    "#画像の中心を切り出し\n",
    "\n",
    "#https://note.nkmk.me/python-pillow-image-crop-trimming/\n",
    "\n",
    "#'''\n",
    "\n",
    "\n",
    "#def crop_center(pil_img, crop_width, crop_height):\n",
    "    \n",
    "#   img_width, img_height = pil_img.size\n",
    "    \n",
    "#    return pil_img.crop(((img_width - crop_width) // 2,\n",
    "#                         (img_height - crop_height) // 2,\n",
    "#                         (img_width + crop_width) // 2,\n",
    "#                         (img_height + crop_height) // 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "prepared-module",
   "metadata": {},
   "outputs": [],
   "source": [
    "#img = Image.open(img_list[1])\n",
    "\n",
    "#img_new = crop_center(img, 224, 224)\n",
    "\n",
    "#print(type(img_new))\n",
    "\n",
    "#plt.imshow(img_new)\n",
    "\n",
    "#plt.show()\n",
    "\n",
    "#img.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inclusive-skating",
   "metadata": {},
   "source": [
    "## 【問題1】自作データセットでの分類の学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "human-clause",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "hollywood-rates",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.imshow(cat_img_array[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "understood-entrepreneur",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 224, 224, 3)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 結合してX_trainに\n",
    "\n",
    "X_train = np.concatenate((cat_img_array, dog_img_array), axis=0)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "auburn-glance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ラベル作成 0=cat, 1=dog\n",
    "\n",
    "y_train = np.array((0,0,0,0,0,1,1,1,1,1))\n",
    "#y_train = y_train.reshape(-1, 1)\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "monthly-recommendation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot\n",
    "#from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "#enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "#y_train = enc.fit_transform(y_train)\n",
    "#y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "difficult-nature",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aquatic-better",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/teruitakahiro/opt/anaconda3/envs/data_science/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/teruitakahiro/opt/anaconda3/envs/data_science/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/teruitakahiro/opt/anaconda3/envs/data_science/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/teruitakahiro/opt/anaconda3/envs/data_science/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/teruitakahiro/opt/anaconda3/envs/data_science/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/teruitakahiro/opt/anaconda3/envs/data_science/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Users/teruitakahiro/opt/anaconda3/envs/data_science/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/teruitakahiro/opt/anaconda3/envs/data_science/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/teruitakahiro/opt/anaconda3/envs/data_science/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/teruitakahiro/opt/anaconda3/envs/data_science/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/teruitakahiro/opt/anaconda3/envs/data_science/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/teruitakahiro/opt/anaconda3/envs/data_science/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "stopped-leeds",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/teruitakahiro/opt/anaconda3/envs/data_science/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "conv_base = tf.keras.applications.vgg16.VGG16(\n",
    "                                                                        weights='imagenet', \n",
    "                                                                        include_top=False, \n",
    "                                                                        input_shape=(224, 224, 3)\n",
    "                                                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "designed-morgan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Model)                (None, 7, 7, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               6422784   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 21,137,729\n",
      "Trainable params: 21,137,729\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#from tf.keras import models\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "model.add(conv_base)\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(256, activation=\"relu\"))\n",
    "model.add(tf.keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "local-adjustment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/teruitakahiro/opt/anaconda3/envs/data_science/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=\"Adam\", loss=\"binary_crossentropy\", metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "possible-trinidad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8 samples, validate on 2 samples\n",
      "Epoch 1/10\n",
      "8/8 [==============================] - 23s 3s/sample - loss: 25.0333 - acc: 0.2500 - val_loss: 0.2047 - val_acc: 1.0000\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 21s 3s/sample - loss: 0.8674 - acc: 0.6250 - val_loss: 0.8365 - val_acc: 0.0000e+00\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 21s 3s/sample - loss: 0.6936 - acc: 0.6250 - val_loss: 1.6036 - val_acc: 0.0000e+00\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 21s 3s/sample - loss: 0.8158 - acc: 0.5000 - val_loss: 0.9616 - val_acc: 0.0000e+00\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 21s 3s/sample - loss: 0.6989 - acc: 0.6250 - val_loss: 0.8857 - val_acc: 0.0000e+00\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - 22s 3s/sample - loss: 0.8061 - acc: 0.7500 - val_loss: 0.0153 - val_acc: 1.0000\n",
      "Epoch 7/10\n",
      "8/8 [==============================] - 24s 3s/sample - loss: 3.8412 - acc: 0.2500 - val_loss: 0.6557 - val_acc: 1.0000\n",
      "Epoch 8/10\n",
      "8/8 [==============================] - 24s 3s/sample - loss: 0.6589 - acc: 0.6250 - val_loss: 0.0063 - val_acc: 1.0000\n",
      "Epoch 9/10\n",
      "8/8 [==============================] - 23s 3s/sample - loss: 2.3195 - acc: 0.7500 - val_loss: 1.2329 - val_acc: 0.0000e+00\n",
      "Epoch 10/10\n",
      "8/8 [==============================] - 25s 3s/sample - loss: 0.7625 - acc: 0.6250 - val_loss: 1.1814 - val_acc: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train,\n",
    "                             batch_size=1,\n",
    "                             epochs=10,\n",
    "                             verbose=1,  # ０=非表示、2=エポックごとの表示\n",
    "                             validation_data=(X_val, y_val)\n",
    "                             )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "widespread-privilege",
   "metadata": {},
   "source": [
    "## 【問題2】分類データセットに対するデータ拡張"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "looking-looking",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                    Version\n",
      "-------------------------- -------------------\n",
      "absl-py                    0.12.0\n",
      "albumentations             0.5.2\n",
      "appnope                    0.1.2\n",
      "argon2-cffi                20.1.0\n",
      "astor                      0.8.1\n",
      "async-generator            1.10\n",
      "attrs                      20.3.0\n",
      "backcall                   0.2.0\n",
      "bleach                     3.3.0\n",
      "cached-property            1.5.2\n",
      "certifi                    2020.12.5\n",
      "cffi                       1.14.3\n",
      "cloudpickle                1.6.0\n",
      "cycler                     0.10.0\n",
      "cytoolz                    0.11.0\n",
      "dask                       2021.3.0\n",
      "decorator                  4.4.2\n",
      "defusedxml                 0.7.1\n",
      "emoji                      0.6.0\n",
      "entrypoints                0.3\n",
      "gast                       0.4.0\n",
      "grpcio                     1.36.1\n",
      "h5py                       3.2.1\n",
      "imageio                    2.9.0\n",
      "imgaug                     0.4.0\n",
      "importlib-metadata         2.0.0\n",
      "ipykernel                  5.3.4\n",
      "ipython                    7.21.0\n",
      "ipython-genutils           0.2.0\n",
      "ipywidgets                 7.5.1\n",
      "jctconv                    0.1.2\n",
      "jedi                       0.17.2\n",
      "Jinja2                     2.11.3\n",
      "joblib                     0.17.0\n",
      "jsonschema                 3.2.0\n",
      "jupyter                    1.0.0\n",
      "jupyter-client             6.1.7\n",
      "jupyter-console            6.2.0\n",
      "jupyter-core               4.7.1\n",
      "jupyterlab-pygments        0.1.2\n",
      "Keras                      2.2.4\n",
      "Keras-Applications         1.0.8\n",
      "Keras-Preprocessing        1.1.2\n",
      "kiwisolver                 1.3.1\n",
      "Markdown                   3.3.4\n",
      "MarkupSafe                 1.1.1\n",
      "matplotlib                 3.3.4\n",
      "mistune                    0.8.4\n",
      "mkl-fft                    1.3.0\n",
      "mkl-random                 1.1.1\n",
      "mkl-service                2.3.0\n",
      "mock                       4.0.3\n",
      "nbclient                   0.5.3\n",
      "nbconvert                  6.0.7\n",
      "nbformat                   5.1.2\n",
      "nest-asyncio               1.5.1\n",
      "networkx                   2.5\n",
      "notebook                   6.2.0\n",
      "numpy                      1.19.3\n",
      "olefile                    0.46\n",
      "opencv-python              4.5.1.48\n",
      "opencv-python-headless     4.5.1.48\n",
      "packaging                  20.9\n",
      "pandas                     1.2.3\n",
      "pandocfilters              1.4.3\n",
      "parso                      0.7.1\n",
      "pexpect                    4.8.0\n",
      "pickleshare                0.7.5\n",
      "Pillow                     8.1.2\n",
      "pip                        20.2.4\n",
      "prometheus-client          0.9.0\n",
      "prompt-toolkit             3.0.8\n",
      "protobuf                   3.15.6\n",
      "ptyprocess                 0.7.0\n",
      "pycparser                  2.20\n",
      "Pygments                   2.8.1\n",
      "pyparsing                  2.4.7\n",
      "pyrsistent                 0.17.3\n",
      "python-dateutil            2.8.1\n",
      "pytz                       2021.1\n",
      "PyWavelets                 1.1.1\n",
      "PyYAML                     5.4.1\n",
      "pyzmq                      20.0.0\n",
      "qtconsole                  4.7.7\n",
      "QtPy                       1.9.0\n",
      "scikit-image               0.18.1\n",
      "scikit-learn               0.23.2\n",
      "scipy                      1.6.2\n",
      "seaborn                    0.11.1\n",
      "Send2Trash                 1.5.0\n",
      "setuptools                 50.3.0.post20201006\n",
      "Shapely                    1.7.1\n",
      "six                        1.15.0\n",
      "sklearn                    0.0\n",
      "tensorboard                1.14.0\n",
      "tensorflow                 1.14.0\n",
      "tensorflow-estimator       1.14.0\n",
      "termcolor                  1.1.0\n",
      "terminado                  0.9.2\n",
      "testpath                   0.4.4\n",
      "threadpoolctl              2.1.0\n",
      "tifffile                   2021.3.17\n",
      "toolz                      0.11.1\n",
      "torch                      1.8.1\n",
      "tornado                    6.1\n",
      "tqdm                       4.59.0\n",
      "traitlets                  5.0.5\n",
      "typing-extensions          3.7.4.3\n",
      "vision-transformer-pytorch 1.0.3\n",
      "wcwidth                    0.2.5\n",
      "webencodings               0.5.1\n",
      "Werkzeug                   1.0.1\n",
      "wheel                      0.35.1\n",
      "widgetsnbextension         3.5.1\n",
      "wrapt                      1.12.1\n",
      "zipp                       3.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "opening-bolivia",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import albumentations as A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "expressed-salvation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from albumentations import (\n",
    "    HorizontalFlip, IAAPerspective, ShiftScaleRotate, CLAHE, RandomRotate90,\n",
    "    Transpose, ShiftScaleRotate, Blur, OpticalDistortion, GridDistortion, HueSaturationValue,\n",
    "    IAAAdditiveGaussianNoise, GaussNoise, MotionBlur, MedianBlur, IAAPiecewiseAffine,\n",
    "    IAASharpen, IAAEmboss, RandomBrightnessContrast, Flip, OneOf, Compose\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "parental-continent",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(image):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "comfortable-stopping",
   "metadata": {},
   "outputs": [],
   "source": [
    " transform = A.Compose([\n",
    "        RandomRotate90(),\n",
    "        Flip(),\n",
    "        Transpose(),\n",
    "        OneOf([\n",
    "            IAAAdditiveGaussianNoise(),\n",
    "            GaussNoise(),\n",
    "        ], p=0.5),\n",
    "        OneOf([\n",
    "            MotionBlur(p=0.2),\n",
    "            MedianBlur(blur_limit=3, p=0.1),\n",
    "            Blur(blur_limit=3, p=0.1),\n",
    "        ], p=0.5),\n",
    "        ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.2, rotate_limit=45, p=0.2),\n",
    "        OneOf([\n",
    "            OpticalDistortion(p=0.3),\n",
    "            GridDistortion(p=0.1),\n",
    "            IAAPiecewiseAffine(p=0.3),\n",
    "        ], p=0.5),\n",
    "        OneOf([\n",
    "            CLAHE(clip_limit=2),\n",
    "            IAASharpen(),\n",
    "            IAAEmboss(),\n",
    "            RandomBrightnessContrast(),\n",
    "        ], p=0.),\n",
    "        HueSaturationValue(p=0.3),\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "dietary-ethernet",
   "metadata": {},
   "outputs": [],
   "source": [
    "# データ拡張\n",
    "\n",
    "image = cv2.imread('./sprint18_image/training/cat.01.jpg')\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "#visualize(image)\n",
    "\n",
    "image.shape\n",
    "augmented_image = transform(image=image)['image']\n",
    "#visualize(augmented_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "armed-banking",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 猫画像を水増しする\n",
    "\n",
    "aug_num = 10\n",
    "\n",
    "for j in range(5):\n",
    "    \n",
    "    image = cv2.imread('./sprint18_image/training/cat.0{}.jpg'.format(j+1))\n",
    "\n",
    "    for i in range(aug_num):\n",
    "        \n",
    "        augmented_image = transform(image=image)['image']\n",
    "        # 可視化する場合\n",
    "        #show_image = cv2.cvtColor(augmented_image, cv2.COLOR_RGB2BGR)\n",
    "        #visualize(show_image)\n",
    "\n",
    "        cv2.imwrite(\"./sprint18_image/aug_train/cat.{}{}.jpg\".format(j, i), augmented_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "needed-leisure",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 犬画像を水増しする\n",
    "\n",
    "aug_num = 10\n",
    "\n",
    "for j in range(5):\n",
    "    \n",
    "    image = cv2.imread('./sprint18_image/training/dog.0{}.jpg'.format(j+1))\n",
    "\n",
    "    for i in range(aug_num):\n",
    "        \n",
    "        augmented_image = transform(image=image)['image']\n",
    "        #show_image = cv2.cvtColor(augmented_image, cv2.COLOR_RGB2BGR)\n",
    "        #visualize(show_image)\n",
    "\n",
    "        cv2.imwrite(\"./sprint18_image/aug_train/dog.{}{}.jpg\".format(j, i), augmented_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "signed-oliver",
   "metadata": {},
   "source": [
    "## 【問題3】物体検出データセットの用意"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electronic-newcastle",
   "metadata": {},
   "source": [
    "### Define functions to visualize bounding boxes and class labels on an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "obvious-privacy",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOX_COLOR = (255, 0, 0) # Red\n",
    "TEXT_COLOR = (255, 255, 255) # White\n",
    "\n",
    "\n",
    "def visualize_bbox(img, bbox, class_name, color=BOX_COLOR, thickness=2):\n",
    "    \n",
    "    \"\"\"Visualizes a single bounding box on the image\"\"\"\n",
    "    #x_min, y_min, w, h = bbox\n",
    "    x_min, y_min, x_max, y_max = bbox\n",
    "    x_min, y_min, x_max, y_max = int(x_min), int(y_min), int(x_max), int(y_max)\n",
    "    #x_min, x_max, y_min, y_max = int(x_min), int(x_min + w), int(y_min), int(y_min + h)\n",
    "    \n",
    "    cv2.rectangle(img, (x_min, y_min), (x_max, y_max), color=color, thickness=thickness)\n",
    "    \n",
    "    ((text_width, text_height), _) = cv2.getTextSize(class_name, cv2.FONT_HERSHEY_SIMPLEX, 0.35, 1)    \n",
    "    cv2.rectangle(img, (x_min, y_min - int(1.3 * text_height)), (x_min + text_width, y_min), BOX_COLOR, -1)\n",
    "    \n",
    "    cv2.putText(\n",
    "                            img,\n",
    "                            text=class_name,\n",
    "                            org=(x_min, y_min - int(0.3 * text_height)),\n",
    "                            fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                            fontScale=0.35, \n",
    "                            color=TEXT_COLOR, \n",
    "                            lineType=cv2.LINE_AA,\n",
    "                            )\n",
    "    \n",
    "    return img\n",
    "\n",
    "\n",
    "\n",
    "def visualize(image, bboxes, category_ids, category_id_to_name):\n",
    "    \n",
    "    img = image.copy()\n",
    "    \n",
    "    for bbox, category_id in zip(bboxes, category_ids):\n",
    "        class_name = category_id_to_name[category_id]\n",
    "        img = visualize_bbox(img, bbox, class_name)\n",
    "        \n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(img)\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "behavioral-yesterday",
   "metadata": {},
   "source": [
    "### Get an image and annotations for it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "injured-logging",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the image from the disk\n",
    "\n",
    "image = cv2.imread('./sprint18_image/training/cat.01.jpg')\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "#plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "changing-proportion",
   "metadata": {},
   "source": [
    "### xmlデータ構造をパース（分析）して座標情報を獲得する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "recent-valuation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['160', '13', '360', '360'],\n",
       " ['177', '192', '815', '971'],\n",
       " ['322', '7', '693', '490'],\n",
       " ['380', '237', '2396', '1702'],\n",
       " ['227', '64', '690', '533'],\n",
       " ['128', '66', '427', '312'],\n",
       " ['115', '24', '1202', '974'],\n",
       " ['182', '72', '715', '509'],\n",
       " ['458', '61', '862', '759'],\n",
       " ['182', '20', '917', '576']]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "\n",
    "# #読み込み例\n",
    "# tree = ET.parse(path + 'cat.41box.xml')\n",
    "# #一番上の階層\n",
    "# root = tree.getroot()\n",
    "\n",
    "COORDINATE = 4\n",
    "\n",
    "path = './sprint18_image/training_bbox_voc/'\n",
    "boxbox = []\n",
    "point = COORDINATE\n",
    "\n",
    "for filename in os.listdir(path):\n",
    "    \n",
    "    boundbox = []    \n",
    "    if not filename.endswith('.xml'): \n",
    "        continue\n",
    "        \n",
    "    fullname = os.path.join(path, filename)\n",
    "    tree = ET.parse(fullname)\n",
    "    \n",
    "    bndbox = tree.findall('object/bndbox')\n",
    "    \n",
    "    for i in range(point):\n",
    "        # 座標取得\n",
    "        boundbox.append(bndbox[0][i].text)\n",
    "    boxbox.append(boundbox)\n",
    "    \n",
    "boxbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "processed-jimmy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define two bounding boxes with coordinates and class labels\n",
    "\n",
    "bboxes = [[160, 13, 360, 360]]\n",
    "category_ids = [0]\n",
    "\n",
    "# We will use the mapping from category_id to the class name\n",
    "# to visualize the class label for the bounding box on the image\n",
    "category_id_to_name = {0: 'cat', 1: 'dog'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "brutal-zimbabwe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visuaize the original image with bounding boxes\n",
    "\n",
    "#visualize(image, bboxes, category_ids, category_id_to_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statistical-croatia",
   "metadata": {},
   "source": [
    "## Sprint18 [問題4] 物体検出用データ拡張"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "rising-sheriff",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = A.Compose(\n",
    "    [A.HorizontalFlip(p=0.5)],\n",
    "    bbox_params=A.BboxParams(format='pascal_voc', label_fields=['category_ids']),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "precise-reliance",
   "metadata": {},
   "outputs": [],
   "source": [
    "#random.seed(7)\n",
    "#transformed = transform(image=image, bboxes=bboxes, category_ids=category_ids)\n",
    "#img = visualize(\n",
    "#    transformed['image'],\n",
    "#    transformed['bboxes'],\n",
    "#    transformed['category_ids'],\n",
    "#    category_id_to_name,\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "false-disorder",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = A.Compose(\n",
    "                                     [A.HorizontalFlip(p=0.5),\n",
    "                                     RandomRotate90(),\n",
    "                                     Transpose(),\n",
    "                                      \n",
    "                                     OneOf([\n",
    "                                                 IAAAdditiveGaussianNoise(),\n",
    "                                                 GaussNoise(),\n",
    "                                                 ], p=0.5),\n",
    "                                      \n",
    "                                    OneOf([\n",
    "                                                MotionBlur(p=0.2),\n",
    "                                                MedianBlur(blur_limit=3, p=0.1),\n",
    "                                                Blur(blur_limit=3, p=0.1),\n",
    "                                                ], p=0.5),\n",
    "                                      \n",
    "                                    OneOf([\n",
    "                                                #OpticalDistortion(p=0.3),\n",
    "                                                #GridDistortion(p=0.1),\n",
    "                                                IAAPiecewiseAffine(p=0.3),\n",
    "                                                ], p=0.5),\n",
    "                                    \n",
    "                                    OneOf([\n",
    "                                                CLAHE(clip_limit=2),\n",
    "                                                IAASharpen(),\n",
    "                                                IAAEmboss(),\n",
    "                                                RandomBrightnessContrast(),\n",
    "                                                ], p=0.),\n",
    "                                    \n",
    "                                    HueSaturationValue(p=0.3), \n",
    "                                    ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.2, rotate_limit=45, p=0.2),\n",
    "                                    ],\n",
    "                                    bbox_params=A.BboxParams(format='pascal_voc', label_fields=['category_ids']),\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "sexual-bedroom",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['160', '13', '360', '360'],\n",
       " ['177', '192', '815', '971'],\n",
       " ['322', '7', '693', '490'],\n",
       " ['380', '237', '2396', '1702'],\n",
       " ['227', '64', '690', '533'],\n",
       " ['128', '66', '427', '312'],\n",
       " ['115', '24', '1202', '974'],\n",
       " ['182', '72', '715', '509'],\n",
       " ['458', '61', '862', '759'],\n",
       " ['182', '20', '917', '576']]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boxbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "entitled-arrow",
   "metadata": {},
   "outputs": [],
   "source": [
    "# データサイズが巨大になるため、以下の処理はコメントアウト\n",
    "\n",
    "# bboxつき猫画像を水増しする\n",
    "# cat01\n",
    "\n",
    "#random.seed()\n",
    "#aug_num = 10\n",
    "#category_id_to_name = {0: 'cat', 1: 'dog'}\n",
    "\n",
    "#bboxes = [[160, 13, 360, 360]]\n",
    "#category_ids = [0] #cat\n",
    "\n",
    "#image = cv2.imread('./sprint18_image/training/cat.01.jpg')\n",
    "\n",
    "#for i in range(aug_num):\n",
    "        \n",
    "#    transformed = transform(image=image, bboxes=bboxes, category_ids=category_ids)\n",
    "#    img = visualize(\n",
    "#                             transformed['image'],\n",
    "#                             transformed['bboxes'],\n",
    "#                             transformed['category_ids'],\n",
    "#                             category_id_to_name,\n",
    "#                             )\n",
    "\n",
    "#    cv2.imwrite(\"./sprint18_image/aug_train_bbox/cat.{0:02d}.jpg\".format(i), img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "appreciated-defense",
   "metadata": {},
   "outputs": [],
   "source": [
    "# データサイズが巨大になるため、以下の処理はコメントアウト\n",
    "\n",
    "# cat02\n",
    "\n",
    "#bboxes = [[177, 192, 815, 971]]\n",
    "#category_ids = [0] #cat\n",
    "#\n",
    "#image = cv2.imread('./sprint18_image/training/cat.02.jpg')\n",
    "#\n",
    "#for i in range(aug_num):\n",
    "#        \n",
    "#    transformed = transform(image=image, bboxes=bboxes, category_ids=category_ids)\n",
    "#    img = visualize(\n",
    "#                             transformed['image'],\n",
    "#                             transformed['bboxes'],\n",
    "#                             transformed['category_ids'],\n",
    "#                             category_id_to_name,\n",
    "#                             )\n",
    "#    \n",
    "#    cv2.imwrite(\"./sprint18_image/aug_train_bbox/cat.1{}.jpg\".format(i), img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "north-williams",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cat03\n",
    "\n",
    "#bboxes = [[322, 7, 693, 490]]\n",
    "#category_ids = [0] #cat\n",
    "\n",
    "#image = cv2.imread('./sprint18_image/training/cat.03.jpg')\n",
    "\n",
    "#for i in range(aug_num):\n",
    "        \n",
    "#    transformed = transform(image=image, bboxes=bboxes, category_ids=category_ids)\n",
    "#   img = visualize(\n",
    "#                             transformed['image'],\n",
    "#                             transformed['bboxes'],\n",
    "#                             transformed['category_ids'],\n",
    "#                             category_id_to_name,\n",
    "#                             )\n",
    "\n",
    "#    cv2.imwrite(\"./sprint18_image/aug_train_bbox/cat.2{}.jpg\".format(i), img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "final-lightweight",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cat04\n",
    "\n",
    "#bboxes =  [[380, 237, 2396, 1702]]\n",
    "#category_ids = [0] #cat\n",
    "\n",
    "#image = cv2.imread('./sprint18_image/training/cat.04.jpg')\n",
    "\n",
    "#for i in range(aug_num):\n",
    "        \n",
    "#    transformed = transform(image=image, bboxes=bboxes, category_ids=category_ids)\n",
    "#    img = visualize(\n",
    "#                             transformed['image'],\n",
    "#                             transformed['bboxes'],\n",
    "#                             transformed['category_ids'],\n",
    "#                             category_id_to_name,\n",
    "#                             )\n",
    "\n",
    "#    cv2.imwrite(\"./sprint18_image/aug_train_bbox/cat.3{}.jpg\".format(i), img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "chemical-jewel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cat05\n",
    "\n",
    "#bboxes =  [[227, 64, 690, 533]]\n",
    "#category_ids = [0] #cat\n",
    "\n",
    "#image = cv2.imread('./sprint18_image/training/cat.05.jpg')\n",
    "\n",
    "#for i in range(aug_num):\n",
    "        \n",
    "#    transformed = transform(image=image, bboxes=bboxes, category_ids=category_ids)\n",
    "#    img = visualize(\n",
    "#                             transformed['image'],\n",
    "#                             transformed['bboxes'],\n",
    "#                             transformed['category_ids'],\n",
    "#                             category_id_to_name,\n",
    "#                             )\n",
    "\n",
    "#    cv2.imwrite(\"./sprint18_image/aug_train_bbox/cat.4{}.jpg\".format(i), img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "serious-england",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dog01\n",
    "\n",
    "#bboxes = [[128, 66, 427, 312]]\n",
    "#category_ids = [1] #dog\n",
    "\n",
    "#image = cv2.imread('./sprint18_image/training/dog.01.jpg')\n",
    "\n",
    "#for i in range(aug_num):\n",
    "        \n",
    "#    transformed = transform(image=image, bboxes=bboxes, category_ids=category_ids)\n",
    "#    img = visualize(\n",
    "#                             transformed['image'],\n",
    "#                             transformed['bboxes'],\n",
    "#                             transformed['category_ids'],\n",
    "#                             category_id_to_name,\n",
    "#                             )\n",
    "\n",
    "#    cv2.imwrite(\"./sprint18_image/aug_train_bbox/dog.{0:02d}.jpg\".format(i), img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "checked-honolulu",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dog02\n",
    "\n",
    "#bboxes = [[115, 24, 1202, 974]]\n",
    "#category_ids = [1] #dog\n",
    "\n",
    "#image = cv2.imread('./sprint18_image/training/dog.02.jpg')\n",
    "\n",
    "#for i in range(aug_num):\n",
    "        \n",
    "#    transformed = transform(image=image, bboxes=bboxes, category_ids=category_ids)\n",
    "#    img = visualize(\n",
    "#                             transformed['image'],\n",
    "#                             transformed['bboxes'],\n",
    "#                             transformed['category_ids'],\n",
    "#                             category_id_to_name,\n",
    "#                             )\n",
    "\n",
    "#    cv2.imwrite(\"./sprint18_image/aug_train_bbox/dog.1{}.jpg\".format(i), img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "wireless-priority",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dog03\n",
    "\n",
    "#bboxes = [[182, 72, 715, 509]]\n",
    "#category_ids = [1] #dog\n",
    "\n",
    "#image = cv2.imread('./sprint18_image/training/dog.03.jpg')\n",
    "\n",
    "#for i in range(aug_num):\n",
    "        \n",
    "#    transformed = transform(image=image, bboxes=bboxes, category_ids=category_ids)\n",
    "#    img = visualize(\n",
    "#                             transformed['image'],\n",
    "#                             transformed['bboxes'],\n",
    "#                             transformed['category_ids'],\n",
    "#                             category_id_to_name,\n",
    "#                             )\n",
    "\n",
    "#    cv2.imwrite(\"./sprint18_image/aug_train_bbox/dog.2{}.jpg\".format(i), img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "heavy-verse",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dog04\n",
    "\n",
    "#bboxes = [[458, 61, 862, 759]]\n",
    "#category_ids = [1] #dog\n",
    "\n",
    "#image = cv2.imread('./sprint18_image/training/dog.04.jpg')\n",
    "\n",
    "#for i in range(aug_num):\n",
    "        \n",
    "#    transformed = transform(image=image, bboxes=bboxes, category_ids=category_ids)\n",
    "#    img = visualize(\n",
    "#                             transformed['image'],\n",
    "#                             transformed['bboxes'],\n",
    "#                             transformed['category_ids'],\n",
    "#                             category_id_to_name,\n",
    "#                             )\n",
    "\n",
    "#    cv2.imwrite(\"./sprint18_image/aug_train_bbox/dog.3{}.jpg\".format(i), img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "smaller-stage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dog05\n",
    "\n",
    "#bboxes = [[182, 20, 917, 576]]\n",
    "#category_ids = [1] #dog\n",
    "\n",
    "#image = cv2.imread('./sprint18_image/training/dog.05.jpg')\n",
    "\n",
    "#for i in range(aug_num):\n",
    "        \n",
    "#    transformed = transform(image=image, bboxes=bboxes, category_ids=category_ids)\n",
    "#    img = visualize(\n",
    "#                             transformed['image'],\n",
    "#                             transformed['bboxes'],\n",
    "#                             transformed['category_ids'],\n",
    "#                             category_id_to_name,\n",
    "#                             )\n",
    "\n",
    "#    cv2.imwrite(\"./sprint18_image/aug_train_bbox/dog.4{}.jpg\".format(i), img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adequate-rebecca",
   "metadata": {},
   "source": [
    "### 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "stainless-istanbul",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cat.00.jpg',\n",
       " 'cat.01.jpg',\n",
       " 'cat.02.jpg',\n",
       " 'cat.03.jpg',\n",
       " 'cat.04.jpg',\n",
       " 'cat.05.jpg',\n",
       " 'cat.06.jpg',\n",
       " 'cat.07.jpg',\n",
       " 'cat.08.jpg',\n",
       " 'cat.09.jpg',\n",
       " 'cat.10.jpg',\n",
       " 'cat.11.jpg',\n",
       " 'cat.12.jpg',\n",
       " 'cat.13.jpg',\n",
       " 'cat.14.jpg',\n",
       " 'cat.15.jpg',\n",
       " 'cat.16.jpg',\n",
       " 'cat.17.jpg',\n",
       " 'cat.18.jpg',\n",
       " 'cat.19.jpg',\n",
       " 'cat.20.jpg',\n",
       " 'cat.21.jpg',\n",
       " 'cat.22.jpg',\n",
       " 'cat.23.jpg',\n",
       " 'cat.24.jpg',\n",
       " 'cat.25.jpg',\n",
       " 'cat.26.jpg',\n",
       " 'cat.27.jpg',\n",
       " 'cat.28.jpg',\n",
       " 'cat.29.jpg',\n",
       " 'cat.30.jpg',\n",
       " 'cat.31.jpg',\n",
       " 'cat.32.jpg',\n",
       " 'cat.33.jpg',\n",
       " 'cat.34.jpg',\n",
       " 'cat.35.jpg',\n",
       " 'cat.36.jpg',\n",
       " 'cat.37.jpg',\n",
       " 'cat.38.jpg',\n",
       " 'cat.39.jpg',\n",
       " 'cat.40.jpg',\n",
       " 'cat.41.jpg',\n",
       " 'cat.42.jpg',\n",
       " 'cat.43.jpg',\n",
       " 'cat.44.jpg',\n",
       " 'cat.45.jpg',\n",
       " 'cat.46.jpg',\n",
       " 'cat.47.jpg',\n",
       " 'cat.48.jpg',\n",
       " 'cat.49.jpg',\n",
       " 'dog.00.jpg',\n",
       " 'dog.01.jpg',\n",
       " 'dog.02.jpg',\n",
       " 'dog.03.jpg',\n",
       " 'dog.04.jpg',\n",
       " 'dog.05.jpg',\n",
       " 'dog.06.jpg',\n",
       " 'dog.07.jpg',\n",
       " 'dog.08.jpg',\n",
       " 'dog.09.jpg',\n",
       " 'dog.10.jpg',\n",
       " 'dog.11.jpg',\n",
       " 'dog.12.jpg',\n",
       " 'dog.13.jpg',\n",
       " 'dog.14.jpg',\n",
       " 'dog.15.jpg',\n",
       " 'dog.16.jpg',\n",
       " 'dog.17.jpg',\n",
       " 'dog.18.jpg',\n",
       " 'dog.19.jpg',\n",
       " 'dog.20.jpg',\n",
       " 'dog.21.jpg',\n",
       " 'dog.22.jpg',\n",
       " 'dog.23.jpg',\n",
       " 'dog.24.jpg',\n",
       " 'dog.25.jpg',\n",
       " 'dog.26.jpg',\n",
       " 'dog.27.jpg',\n",
       " 'dog.28.jpg',\n",
       " 'dog.29.jpg',\n",
       " 'dog.30.jpg',\n",
       " 'dog.31.jpg',\n",
       " 'dog.32.jpg',\n",
       " 'dog.33.jpg',\n",
       " 'dog.34.jpg',\n",
       " 'dog.35.jpg',\n",
       " 'dog.36.jpg',\n",
       " 'dog.37.jpg',\n",
       " 'dog.38.jpg',\n",
       " 'dog.39.jpg',\n",
       " 'dog.40.jpg',\n",
       " 'dog.41.jpg',\n",
       " 'dog.42.jpg',\n",
       " 'dog.43.jpg',\n",
       " 'dog.44.jpg',\n",
       " 'dog.45.jpg',\n",
       " 'dog.46.jpg',\n",
       " 'dog.47.jpg',\n",
       " 'dog.48.jpg',\n",
       " 'dog.49.jpg']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = './sprint18_image/aug_train_bbox/'\n",
    "flist = os.listdir(path)\n",
    "flist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "engaged-being",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./sprint18_image/aug_train_bbox/cat.00.jpg',\n",
       " './sprint18_image/aug_train_bbox/cat.01.jpg',\n",
       " './sprint18_image/aug_train_bbox/cat.02.jpg',\n",
       " './sprint18_image/aug_train_bbox/cat.03.jpg',\n",
       " './sprint18_image/aug_train_bbox/cat.04.jpg',\n",
       " './sprint18_image/aug_train_bbox/cat.05.jpg',\n",
       " './sprint18_image/aug_train_bbox/cat.06.jpg',\n",
       " './sprint18_image/aug_train_bbox/cat.07.jpg',\n",
       " './sprint18_image/aug_train_bbox/cat.08.jpg',\n",
       " './sprint18_image/aug_train_bbox/cat.09.jpg',\n",
       " './sprint18_image/aug_train_bbox/cat.10.jpg',\n",
       " './sprint18_image/aug_train_bbox/cat.11.jpg',\n",
       " './sprint18_image/aug_train_bbox/cat.12.jpg',\n",
       " './sprint18_image/aug_train_bbox/cat.13.jpg',\n",
       " './sprint18_image/aug_train_bbox/cat.14.jpg',\n",
       " './sprint18_image/aug_train_bbox/cat.15.jpg',\n",
       " './sprint18_image/aug_train_bbox/cat.16.jpg',\n",
       " './sprint18_image/aug_train_bbox/cat.17.jpg',\n",
       " './sprint18_image/aug_train_bbox/cat.18.jpg',\n",
       " './sprint18_image/aug_train_bbox/cat.19.jpg',\n",
       " './sprint18_image/aug_train_bbox/cat.20.jpg',\n",
       " './sprint18_image/aug_train_bbox/cat.21.jpg',\n",
       " './sprint18_image/aug_train_bbox/cat.22.jpg',\n",
       " './sprint18_image/aug_train_bbox/cat.23.jpg',\n",
       " './sprint18_image/aug_train_bbox/cat.24.jpg',\n",
       " './sprint18_image/aug_train_bbox/cat.25.jpg',\n",
       " './sprint18_image/aug_train_bbox/cat.26.jpg',\n",
       " './sprint18_image/aug_train_bbox/cat.27.jpg',\n",
       " './sprint18_image/aug_train_bbox/cat.28.jpg',\n",
       " './sprint18_image/aug_train_bbox/cat.29.jpg',\n",
       " './sprint18_image/aug_train_bbox/cat.30.jpg',\n",
       " './sprint18_image/aug_train_bbox/cat.31.jpg',\n",
       " './sprint18_image/aug_train_bbox/cat.32.jpg',\n",
       " './sprint18_image/aug_train_bbox/cat.33.jpg',\n",
       " './sprint18_image/aug_train_bbox/cat.34.jpg',\n",
       " './sprint18_image/aug_train_bbox/cat.35.jpg',\n",
       " './sprint18_image/aug_train_bbox/cat.36.jpg',\n",
       " './sprint18_image/aug_train_bbox/cat.37.jpg',\n",
       " './sprint18_image/aug_train_bbox/cat.38.jpg',\n",
       " './sprint18_image/aug_train_bbox/cat.39.jpg',\n",
       " './sprint18_image/aug_train_bbox/cat.40.jpg',\n",
       " './sprint18_image/aug_train_bbox/cat.41.jpg',\n",
       " './sprint18_image/aug_train_bbox/cat.42.jpg',\n",
       " './sprint18_image/aug_train_bbox/cat.43.jpg',\n",
       " './sprint18_image/aug_train_bbox/cat.44.jpg',\n",
       " './sprint18_image/aug_train_bbox/cat.45.jpg',\n",
       " './sprint18_image/aug_train_bbox/cat.46.jpg',\n",
       " './sprint18_image/aug_train_bbox/cat.47.jpg',\n",
       " './sprint18_image/aug_train_bbox/cat.48.jpg',\n",
       " './sprint18_image/aug_train_bbox/cat.49.jpg',\n",
       " './sprint18_image/aug_train_bbox/dog.00.jpg',\n",
       " './sprint18_image/aug_train_bbox/dog.01.jpg',\n",
       " './sprint18_image/aug_train_bbox/dog.02.jpg',\n",
       " './sprint18_image/aug_train_bbox/dog.03.jpg',\n",
       " './sprint18_image/aug_train_bbox/dog.04.jpg',\n",
       " './sprint18_image/aug_train_bbox/dog.05.jpg',\n",
       " './sprint18_image/aug_train_bbox/dog.06.jpg',\n",
       " './sprint18_image/aug_train_bbox/dog.07.jpg',\n",
       " './sprint18_image/aug_train_bbox/dog.08.jpg',\n",
       " './sprint18_image/aug_train_bbox/dog.09.jpg',\n",
       " './sprint18_image/aug_train_bbox/dog.10.jpg',\n",
       " './sprint18_image/aug_train_bbox/dog.11.jpg',\n",
       " './sprint18_image/aug_train_bbox/dog.12.jpg',\n",
       " './sprint18_image/aug_train_bbox/dog.13.jpg',\n",
       " './sprint18_image/aug_train_bbox/dog.14.jpg',\n",
       " './sprint18_image/aug_train_bbox/dog.15.jpg',\n",
       " './sprint18_image/aug_train_bbox/dog.16.jpg',\n",
       " './sprint18_image/aug_train_bbox/dog.17.jpg',\n",
       " './sprint18_image/aug_train_bbox/dog.18.jpg',\n",
       " './sprint18_image/aug_train_bbox/dog.19.jpg',\n",
       " './sprint18_image/aug_train_bbox/dog.20.jpg',\n",
       " './sprint18_image/aug_train_bbox/dog.21.jpg',\n",
       " './sprint18_image/aug_train_bbox/dog.22.jpg',\n",
       " './sprint18_image/aug_train_bbox/dog.23.jpg',\n",
       " './sprint18_image/aug_train_bbox/dog.24.jpg',\n",
       " './sprint18_image/aug_train_bbox/dog.25.jpg',\n",
       " './sprint18_image/aug_train_bbox/dog.26.jpg',\n",
       " './sprint18_image/aug_train_bbox/dog.27.jpg',\n",
       " './sprint18_image/aug_train_bbox/dog.28.jpg',\n",
       " './sprint18_image/aug_train_bbox/dog.29.jpg',\n",
       " './sprint18_image/aug_train_bbox/dog.30.jpg',\n",
       " './sprint18_image/aug_train_bbox/dog.31.jpg',\n",
       " './sprint18_image/aug_train_bbox/dog.32.jpg',\n",
       " './sprint18_image/aug_train_bbox/dog.33.jpg',\n",
       " './sprint18_image/aug_train_bbox/dog.34.jpg',\n",
       " './sprint18_image/aug_train_bbox/dog.35.jpg',\n",
       " './sprint18_image/aug_train_bbox/dog.36.jpg',\n",
       " './sprint18_image/aug_train_bbox/dog.37.jpg',\n",
       " './sprint18_image/aug_train_bbox/dog.38.jpg',\n",
       " './sprint18_image/aug_train_bbox/dog.39.jpg',\n",
       " './sprint18_image/aug_train_bbox/dog.40.jpg',\n",
       " './sprint18_image/aug_train_bbox/dog.41.jpg',\n",
       " './sprint18_image/aug_train_bbox/dog.42.jpg',\n",
       " './sprint18_image/aug_train_bbox/dog.43.jpg',\n",
       " './sprint18_image/aug_train_bbox/dog.44.jpg',\n",
       " './sprint18_image/aug_train_bbox/dog.45.jpg',\n",
       " './sprint18_image/aug_train_bbox/dog.46.jpg',\n",
       " './sprint18_image/aug_train_bbox/dog.47.jpg',\n",
       " './sprint18_image/aug_train_bbox/dog.48.jpg',\n",
       " './sprint18_image/aug_train_bbox/dog.49.jpg']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_list = glob.glob(path + '/*' + \".jpg\")\n",
    "img_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "innovative-canyon",
   "metadata": {},
   "outputs": [],
   "source": [
    "dog_img_array = np.empty((0,224,224,3))\n",
    "cat_img_array = np.empty((0,224,224,3))\n",
    "\n",
    "for img in img_list:\n",
    "    \n",
    "    if re.search('dog', img):\n",
    "        \n",
    "        dog_img_ = Image.open(img)\n",
    "        dog_img_ = dog_img_.resize((224, 224))\n",
    "        dog_img = np.array(dog_img_)\n",
    "        dog_img = dog_img / 255.\n",
    "        dog_img = dog_img.reshape((1,224,224,3))\n",
    "        dog_img_array = np.concatenate([dog_img_array, dog_img], axis = 0)\n",
    "        dog_img_.close()\n",
    "    \n",
    "    if re.search('cat', img):\n",
    "        \n",
    "        cat_img_ = Image.open(img)\n",
    "        cat_img_ = cat_img_.resize((224, 224))\n",
    "        cat_img = np.array(cat_img_)\n",
    "        cat_img = cat_img / 255.\n",
    "        cat_img = cat_img.reshape((1,224,224,3))\n",
    "        cat_img_array = np.concatenate([cat_img_array, cat_img], axis = 0)\n",
    "        cat_img_.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "antique-secondary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog_image:(50, 224, 224, 3)  cat_image:(50, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "print('dog_image:{}  cat_image:{}'.format(dog_img_array.shape, cat_img_array.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "grave-regard",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 224, 224, 3)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 結合してX_trainに\n",
    "\n",
    "X_train = np.concatenate((cat_img_array, dog_img_array), axis=0)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "personal-abraham",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ラベル作成 0=cat, 1=dog\n",
    "\n",
    "label_cat = np.zeros((len(cat_img_array)))\n",
    "label_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "built-brunswick",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_dog = np.ones((len(dog_img_array)))\n",
    "label_dog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "acute-environment",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = np.concatenate((label_cat, label_dog))\n",
    "#y_train = y_train.reshape(-1, 1)\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ambient-provincial",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot\n",
    "#from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "#enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "#y_train = enc.fit_transform(y_train)\n",
    "#y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "minute-judge",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "seventh-filling",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_base = tf.keras.applications.vgg16.VGG16(\n",
    "                                                                        weights='imagenet', \n",
    "                                                                        include_top=False, \n",
    "                                                                        input_shape=(224, 224, 3)\n",
    "                                                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ruled-louisville",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Model)                (None, 7, 7, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               6422784   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 21,137,729\n",
      "Trainable params: 21,137,729\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#from tf.keras import models\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "model.add(conv_base)\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(256, activation=\"relu\"))\n",
    "model.add(tf.keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "therapeutic-bonus",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"Adam\", loss=\"binary_crossentropy\", metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "productive-university",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 80 samples, validate on 20 samples\n",
      "Epoch 1/10\n",
      "80/80 [==============================] - 255s 3s/sample - loss: 7.5820 - acc: 0.4625 - val_loss: 0.7251 - val_acc: 0.3500\n",
      "Epoch 2/10\n",
      "80/80 [==============================] - 270s 3s/sample - loss: 0.7045 - acc: 0.4375 - val_loss: 0.6938 - val_acc: 0.3500\n",
      "Epoch 3/10\n",
      "80/80 [==============================] - 279s 3s/sample - loss: 0.6935 - acc: 0.4625 - val_loss: 0.6936 - val_acc: 0.3500\n",
      "Epoch 4/10\n",
      "80/80 [==============================] - 246s 3s/sample - loss: 0.6931 - acc: 0.5375 - val_loss: 0.6945 - val_acc: 0.3500\n",
      "Epoch 5/10\n",
      "80/80 [==============================] - 243s 3s/sample - loss: 0.6927 - acc: 0.5375 - val_loss: 0.6963 - val_acc: 0.3500\n",
      "Epoch 6/10\n",
      "80/80 [==============================] - 235s 3s/sample - loss: 0.6926 - acc: 0.5375 - val_loss: 0.6972 - val_acc: 0.3500\n",
      "Epoch 7/10\n",
      "80/80 [==============================] - 228s 3s/sample - loss: 0.6925 - acc: 0.5375 - val_loss: 0.6990 - val_acc: 0.3500\n",
      "Epoch 8/10\n",
      "80/80 [==============================] - 229s 3s/sample - loss: 0.6925 - acc: 0.5375 - val_loss: 0.6990 - val_acc: 0.3500\n",
      "Epoch 9/10\n",
      "80/80 [==============================] - 268s 3s/sample - loss: 0.6921 - acc: 0.5375 - val_loss: 0.7005 - val_acc: 0.3500\n",
      "Epoch 10/10\n",
      "80/80 [==============================] - 248s 3s/sample - loss: 0.6921 - acc: 0.5375 - val_loss: 0.7015 - val_acc: 0.3500\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train,\n",
    "                             batch_size=1,\n",
    "                             epochs=10,\n",
    "                             verbose=1,  # ０=非表示、2=エポックごとの表示\n",
    "                             validation_data=(X_val, y_val)\n",
    "                             )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ordered-karen",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "visible-federal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.DS_Store',\n",
       " '.ipynb_checkpoints',\n",
       " 'cat.01.jpg',\n",
       " 'cat.02.jpg',\n",
       " 'cat.03.jpg',\n",
       " 'dog.01.jpg',\n",
       " 'dog.02.jpg',\n",
       " 'dog.03.jpg']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = './sprint18_image/testdataset/'\n",
    "flist = os.listdir(path)\n",
    "flist\n",
    "\n",
    "# チャネル数４のデータは排除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "available-decimal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./sprint18_image/testdataset/cat.01.jpg',\n",
       " './sprint18_image/testdataset/cat.02.jpg',\n",
       " './sprint18_image/testdataset/cat.03.jpg',\n",
       " './sprint18_image/testdataset/dog.01.jpg',\n",
       " './sprint18_image/testdataset/dog.02.jpg',\n",
       " './sprint18_image/testdataset/dog.03.jpg']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_list = glob.glob(path + '/*' + \".jpg\")\n",
    "img_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "standard-virgin",
   "metadata": {},
   "outputs": [],
   "source": [
    "dog_img_array = np.empty((0,224,224,3))\n",
    "cat_img_array = np.empty((0,224,224,3))\n",
    "\n",
    "for img in img_list:\n",
    "    \n",
    "    if re.search('dog', img):\n",
    "        \n",
    "        dog_img_ = Image.open(img)\n",
    "        dog_img_ = dog_img_.resize((224, 224))\n",
    "        dog_img = np.array(dog_img_)\n",
    "        #print(dog_img.shape)\n",
    "        dog_img = dog_img / 255.\n",
    "        dog_img = dog_img.reshape((1,224,224,3))\n",
    "        dog_img_array = np.concatenate([dog_img_array, dog_img], axis = 0)\n",
    "        dog_img_.close()\n",
    "    \n",
    "    if re.search('cat', img):\n",
    "        \n",
    "        cat_img_ = Image.open(img)\n",
    "        cat_img_ = cat_img_.resize((224, 224))\n",
    "        cat_img = np.array(cat_img_)\n",
    "        cat_img = cat_img / 255.\n",
    "        cat_img = cat_img.reshape((1,224,224,3))\n",
    "        cat_img_array = np.concatenate([cat_img_array, cat_img], axis = 0)\n",
    "        cat_img_.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "sacred-fountain",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog_image:(3, 224, 224, 3)  cat_image:(3, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "print('dog_image:{}  cat_image:{}'.format(dog_img_array.shape, cat_img_array.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "spread-storm",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 224, 224, 3)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = np.concatenate((cat_img_array, dog_img_array), axis=0)\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "genuine-newark",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "6/6 [==============================] - 7s 1s/sample\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.5134023 ],\n",
       "       [0.5134023 ],\n",
       "       [0.5134023 ],\n",
       "       [0.5134023 ],\n",
       "       [0.51340234],\n",
       "       [0.51340234]], dtype=float32)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 精度上がらず\n",
    "\n",
    "model.predict(X_test, batch_size=None, verbose=1, steps=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "hazardous-grill",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./sprint18_image/testdataset/dog.03.jpg'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conservative-inclusion",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
