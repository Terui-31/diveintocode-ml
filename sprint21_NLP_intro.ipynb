{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "wanted-generation",
   "metadata": {},
   "source": [
    "### IMDB映画レビューデータセットの準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "arbitrary-theta",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-04-07 09:43:20--  http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
      "ai.stanford.edu (ai.stanford.edu) をDNSに問いあわせています... 171.64.68.10\n",
      "ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:80 に接続しています... 接続しました。\n",
      "HTTP による接続要求を送信しました、応答を待っています... 200 OK\n",
      "長さ: 84125825 (80M) [application/x-gzip]\n",
      "`aclImdb_v1.tar.gz' に保存中\n",
      "\n",
      "aclImdb_v1.tar.gz   100%[===================>]  80.23M  5.97MB/s 時間 15s        \n",
      "\n",
      "2021-04-07 09:43:36 (5.36 MB/s) - `aclImdb_v1.tar.gz' へ保存完了 [84125825/84125825]\n",
      "\n",
      "Large Movie Review Dataset v1.0\n",
      "\n",
      "Overview\n",
      "\n",
      "This dataset contains movie reviews along with their associated binary\n",
      "sentiment polarity labels. It is intended to serve as a benchmark for\n",
      "sentiment classification. This document outlines how the dataset was\n",
      "gathered, and how to use the files provided. \n",
      "\n",
      "Dataset \n",
      "\n",
      "The core dataset contains 50,000 reviews split evenly into 25k train\n",
      "and 25k test sets. The overall distribution of labels is balanced (25k\n",
      "pos and 25k neg). We also include an additional 50,000 unlabeled\n",
      "documents for unsupervised learning. \n",
      "\n",
      "In the entire collection, no more than 30 reviews are allowed for any\n",
      "given movie because reviews for the same movie tend to have correlated\n",
      "ratings. Further, the train and test sets contain a disjoint set of\n",
      "movies, so no significant performance is obtained by memorizing\n",
      "movie-unique terms and their associated with observed labels.  In the\n",
      "labeled train/test sets, a negative review has a score <= 4 out of 10,\n",
      "and a positive review has a score >= 7 out of 10. Thus reviews with\n",
      "more neutral ratings are not included in the train/test sets. In the\n",
      "unsupervised set, reviews of any rating are included and there are an\n",
      "even number of reviews > 5 and <= 5.\n",
      "\n",
      "Files\n",
      "\n",
      "There are two top-level directories [train/, test/] corresponding to\n",
      "the training and test sets. Each contains [pos/, neg/] directories for\n",
      "the reviews with binary labels positive and negative. Within these\n",
      "directories, reviews are stored in text files named following the\n",
      "convention [[id]_[rating].txt] where [id] is a unique id and [rating] is\n",
      "the star rating for that review on a 1-10 scale. For example, the file\n",
      "[test/pos/200_8.txt] is the text for a positive-labeled test set\n",
      "example with unique id 200 and star rating 8/10 from IMDb. The\n",
      "[train/unsup/] directory has 0 for all ratings because the ratings are\n",
      "omitted for this portion of the dataset.\n",
      "\n",
      "We also include the IMDb URLs for each review in a separate\n",
      "[urls_[pos, neg, unsup].txt] file. A review with unique id 200 will\n",
      "have its URL on line 200 of this file. Due the ever-changing IMDb, we\n",
      "are unable to link directly to the review, but only to the movie's\n",
      "review page.\n",
      "\n",
      "In addition to the review text files, we include already-tokenized bag\n",
      "of words (BoW) features that were used in our experiments. These \n",
      "are stored in .feat files in the train/test directories. Each .feat\n",
      "file is in LIBSVM format, an ascii sparse-vector format for labeled\n",
      "data.  The feature indices in these files start from 0, and the text\n",
      "tokens corresponding to a feature index is found in [imdb.vocab]. So a\n",
      "line with 0:7 in a .feat file means the first word in [imdb.vocab]\n",
      "(the) appears 7 times in that review.\n",
      "\n",
      "LIBSVM page for details on .feat file format:\n",
      "http://www.csie.ntu.edu.tw/~cjlin/libsvm/\n",
      "\n",
      "We also include [imdbEr.txt] which contains the expected rating for\n",
      "each token in [imdb.vocab] as computed by (Potts, 2011). The expected\n",
      "rating is a good way to get a sense for the average polarity of a word\n",
      "in the dataset.\n",
      "\n",
      "Citing the dataset\n",
      "\n",
      "When using this dataset please cite our ACL 2011 paper which\n",
      "introduces it. This paper also contains classification results which\n",
      "you may want to compare against.\n",
      "\n",
      "\n",
      "@InProceedings{maas-EtAl:2011:ACL-HLT2011,\n",
      "  author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\n",
      "  title     = {Learning Word Vectors for Sentiment Analysis},\n",
      "  booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n",
      "  month     = {June},\n",
      "  year      = {2011},\n",
      "  address   = {Portland, Oregon, USA},\n",
      "  publisher = {Association for Computational Linguistics},\n",
      "  pages     = {142--150},\n",
      "  url       = {http://www.aclweb.org/anthology/P11-1015}\n",
      "}\n",
      "\n",
      "References\n",
      "\n",
      "Potts, Christopher. 2011. On the negativity of negation. In Nan Li and\n",
      "David Lutz, eds., Proceedings of Semantics and Linguistic Theory 20,\n",
      "636-659.\n",
      "\n",
      "Contact\n",
      "\n",
      "For questions/comments/corrections please contact Andrew Maas\n",
      "amaas@cs.stanford.edu\n"
     ]
    }
   ],
   "source": [
    "# IMDBをカレントフォルダにダウンロード\n",
    "!wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "# 解凍\n",
    "!tar zxf aclImdb_v1.tar.gz\n",
    "# aclImdb/train/unsupはラベル無しのため削除\n",
    "!rm -rf aclImdb/train/unsup\n",
    "# IMDBデータセットの説明を表示\n",
    "!cat aclImdb/README"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "hungarian-hepatitis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sprint6_decision_tree_draft.ipynb  sprint9_FNN_draft.ipynb\r\n",
      "Week1_01_prev.ipynb                week1_prod_01.ipynb\r\n",
      "Week1_03 (1_7) .ipynb              week1_prod_02.ipynb\r\n",
      "\u001b[34maclImdb\u001b[m\u001b[m/                           week1_prod_03.ipynb\r\n",
      "aclImdb_v1.tar.gz                  week2_prev_01.ipynb\r\n",
      "sprint21_NLP_draft.ipynb           week2_prev_02.ipynb\r\n",
      "sprint3_linear_reg_draft.ipynb     week2_prev_04.ipynb\r\n",
      "sprint8_ensemble_draft.ipynb       week3_prev_01_draft.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "partial-wheel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neg', 'pos']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files\n",
    "\n",
    "train_review = load_files('./aclImdb/train/', encoding='utf-8')\n",
    "x_train, y_train = train_review.data, train_review.target\n",
    "\n",
    "test_review = load_files('./aclImdb/test/', encoding='utf-8')\n",
    "x_test, y_test = test_review.data, test_review.target\n",
    "\n",
    "# ラベルの0,1と意味の対応の表示\n",
    "print(train_review.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "empirical-library",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x : Words can't describe how bad this movie is. I can't explain it by writing only. You have too see it for yourself to get at grip of how horrible a movie really can be. Not that I recommend you to do that. There are so many clichés, mistakes (and all other negative things you can imagine) here that will just make you cry. To start with the technical first, there are a LOT of mistakes regarding the airplane. I won't list them here, but just mention the coloring of the plane. They didn't even manage to show an airliner in the colors of a fictional airline, but instead used a 747 painted in the original Boeing livery. Very bad. The plot is stupid and has been done many times before, only much, much better. There are so many ridiculous moments here that i lost count of it really early. Also, I was on the bad guys' side all the time in the movie, because the good guys were so stupid. \"Executive Decision\" should without a doubt be you're choice over this one, even the \"Turbulence\"-movies are better. In fact, every other movie in the world is better than this one.\n"
     ]
    }
   ],
   "source": [
    "print(\"x : {}\".format(x_train[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nutritional-there",
   "metadata": {},
   "source": [
    "# BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "blond-utility",
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_dataset = \\\n",
    "  [\"This movie is very good.\",\n",
    "  \"This film is a good\",\n",
    "  \"Very bad. Very, very bad.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "smart-parcel",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>bad</th>\n",
       "      <th>film</th>\n",
       "      <th>good</th>\n",
       "      <th>is</th>\n",
       "      <th>movie</th>\n",
       "      <th>this</th>\n",
       "      <th>very</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a  bad  film  good  is  movie  this  very\n",
       "0  0    0     0     1   1      1     1     1\n",
       "1  1    0     1     1   1      0     1     0\n",
       "2  0    2     0     0   0      0     0     3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# BoW は単語の登場回数のOne-hot表現\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(token_pattern=r'(?u)\\b\\w+\\b')\n",
    "bow = (vectorizer.fit_transform(mini_dataset)).toarray()\n",
    "\n",
    "# DataFrameにまとめる\n",
    "df = pd.DataFrame(bow, columns=vectorizer.get_feature_names())\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "furnished-mentor",
   "metadata": {},
   "source": [
    "### N-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "threaded-bidder",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a good</th>\n",
       "      <th>bad very</th>\n",
       "      <th>film is</th>\n",
       "      <th>is a</th>\n",
       "      <th>is very</th>\n",
       "      <th>movie is</th>\n",
       "      <th>this film</th>\n",
       "      <th>this movie</th>\n",
       "      <th>very bad</th>\n",
       "      <th>very good</th>\n",
       "      <th>very very</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a good  bad very  film is  is a  is very  movie is  this film  this movie  \\\n",
       "0       0         0        0     0        1         1          0           1   \n",
       "1       1         0        1     1        0         0          1           0   \n",
       "2       0         1        0     0        0         0          0           0   \n",
       "\n",
       "   very bad  very good  very very  \n",
       "0         0          1          0  \n",
       "1         0          0          0  \n",
       "2         2          0          1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ngram_rangeで利用するn-gramの範囲を指定する\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(2, 2), token_pattern=r'(?u)\\b\\w+\\b')\n",
    "bow_train = (vectorizer.fit_transform(mini_dataset)).toarray()\n",
    "df = pd.DataFrame(bow_train, columns=vectorizer.get_feature_names())\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "residential-check",
   "metadata": {},
   "source": [
    "## 【問題1】BoWのスクラッチ実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ongoing-hostel",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "adequate-conversation",
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_dataset2 = \\\n",
    "                          [\"This movie is SOOOO funny!!!\",\n",
    "                           \"What a movie! I never\",\n",
    "                           \"best movie ever!!!!! this movie\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "analyzed-durham",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This movie is SOOOO funny!!!'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mini_dataset2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "logical-landing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'movie', 'is', 'SOOOO', 'funny!!!']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mini_dataset2[0].split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "furnished-movie",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'movie', 'is', 'SOOOO', 'funny!!!']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mini_dataset2[0].split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "stylish-happening",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'movie', 'is', 'SOOOO', 'funny']"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence0 = mini_dataset2[0].replace(\"!\", \"\").split(\" \")\n",
    "sentence0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "mighty-violation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What', 'a', 'movie', 'I', 'never']"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence1 = mini_dataset2[1].replace(\"!\", \"\").split(\" \")\n",
    "sentence1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "parental-villa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['best', 'movie', 'ever', 'this', 'movie']"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence2 = mini_dataset2[2].replace(\"!\", \"\").split(\" \")\n",
    "sentence2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "affecting-habitat",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'movie',\n",
       " 'is',\n",
       " 'SOOOO',\n",
       " 'funny',\n",
       " 'What',\n",
       " 'a',\n",
       " 'movie',\n",
       " 'I',\n",
       " 'never',\n",
       " 'best',\n",
       " 'movie',\n",
       " 'ever',\n",
       " 'this',\n",
       " 'movie']"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence0 + sentence1 +  sentence2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "municipal-partnership",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'I',\n",
       " 'SOOOO',\n",
       " 'This',\n",
       " 'What',\n",
       " 'a',\n",
       " 'best',\n",
       " 'ever',\n",
       " 'funny',\n",
       " 'is',\n",
       " 'movie',\n",
       " 'never',\n",
       " 'this'}"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set で重複を削除、リストではなくなる\n",
    "\n",
    "set(sentence0 + sentence1 +  sentence2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "pointed-monkey",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['movie',\n",
       " 'I',\n",
       " 'SOOOO',\n",
       " 'funny',\n",
       " 'This',\n",
       " 'never',\n",
       " 'best',\n",
       " 'this',\n",
       " 'ever',\n",
       " 'a',\n",
       " 'is',\n",
       " 'What']"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_list = list(set(sentence0 + sentence1 +  sentence2))\n",
    "word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "indoor-letter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'SOOOO',\n",
       " 'This',\n",
       " 'What',\n",
       " 'a',\n",
       " 'best',\n",
       " 'ever',\n",
       " 'funny',\n",
       " 'is',\n",
       " 'movie',\n",
       " 'never',\n",
       " 'this']"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_list.sort()\n",
    "word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "stock-ground",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out0 = np.zeros((len(word_list)))\n",
    "out0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "included-trouble",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if sentence0[0]==word_list[0]:\n",
    "    out0 += 1\n",
    "\n",
    "out0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "atomic-freeware",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# column1[0]==\"This\" があったところに１を足す\n",
    "\n",
    "out0 = np.zeros((len(word_list)))\n",
    "\n",
    "for i, word in enumerate(word_list):\n",
    "    \n",
    "    if sentence0[0]==word:\n",
    "        out0[i] += 1\n",
    "        \n",
    "out0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "greater-quebec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0.])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sentence0 について\n",
    "\n",
    "out0 = np.zeros((len(word_list)))\n",
    "\n",
    "for sentence in sentence0:\n",
    "    \n",
    "    for i, word in enumerate(word_list):\n",
    "    \n",
    "        if sentence==word:\n",
    "            out0[i] += 1\n",
    "            \n",
    "out0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "approximate-hurricane",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3, 12),\n",
       " array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]))"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_counts = 3\n",
    "\n",
    "out = np.zeros((sentence_counts, len(word_list)))\n",
    "\n",
    "out.shape, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "difficult-serve",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This movie is SOOOO funny!!!'"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mini_dataset2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "scheduled-holiday",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_counts = 3\n",
    "out = np.zeros((sentence_counts, len(word_list)))\n",
    "\n",
    "\n",
    "for sentence in mini_dataset2[0].split(\" \"):\n",
    "    \n",
    "    for i, word in enumerate(word_list):\n",
    "    \n",
    "        if sentence==word:\n",
    "            out[0, i] += 1\n",
    "            \n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "cloudy-effort",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0],\n",
       "       [1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 1]])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_counts = 3\n",
    "out = np.zeros((sentence_counts, len(word_list)))\n",
    "\n",
    "for j in range(sentence_counts):\n",
    "\n",
    "    for sentence in mini_dataset2[j].split(\" \"):\n",
    "\n",
    "        for i, word in enumerate(word_list):\n",
    "\n",
    "            if sentence==word:\n",
    "                out[j, i] += 1\n",
    "            \n",
    "out = out.astype(np.int)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "federal-military",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>I</th>\n",
       "      <th>SOOOO</th>\n",
       "      <th>This</th>\n",
       "      <th>What</th>\n",
       "      <th>a</th>\n",
       "      <th>best</th>\n",
       "      <th>ever</th>\n",
       "      <th>funny</th>\n",
       "      <th>is</th>\n",
       "      <th>movie</th>\n",
       "      <th>never</th>\n",
       "      <th>this</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   I  SOOOO  This  What  a  best  ever  funny  is  movie  never  this\n",
       "0  0      1     1     0  0     0     0      0   1      1      0     0\n",
       "1  1      0     0     1  1     0     0      0   0      0      1     0\n",
       "2  0      0     0     0  0     1     0      0   0      2      0     1"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df =  pd.DataFrame(out, columns=word_list)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "talented-violin",
   "metadata": {},
   "source": [
    "### uni-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "id": "skilled-football",
   "metadata": {},
   "outputs": [],
   "source": [
    "def uni_gram(X):\n",
    "\n",
    "    \"\"\"\n",
    "    listを受け取りuni-gramのBoWを返す\n",
    "    \"\"\"\n",
    "    \n",
    "    word_list = []\n",
    "\n",
    "    for i in range(len(X)):\n",
    "\n",
    "        split = X[i].replace(\"!\", \"\").split(\" \")\n",
    "        word_list.extend(split)\n",
    "        \n",
    "    word_list = list(set(word_list))\n",
    "    word_list.sort()\n",
    "    sentence_counts = len(X)\n",
    "    out = np.zeros((sentence_counts, len(word_list)))\n",
    "\n",
    "\n",
    "    for j in range(sentence_counts):\n",
    "\n",
    "        for sentence in X[j].replace(\"!\", \"\").split(\" \"):\n",
    "\n",
    "            for i, word in enumerate(word_list):\n",
    "\n",
    "                if sentence==word:\n",
    "                    out[j, i] += 1\n",
    "\n",
    "                    \n",
    "    out = out.astype(np.int)\n",
    "    df =  pd.DataFrame(out, columns=word_list)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "similar-shuttle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>I</th>\n",
       "      <th>SOOOO</th>\n",
       "      <th>This</th>\n",
       "      <th>What</th>\n",
       "      <th>a</th>\n",
       "      <th>best</th>\n",
       "      <th>ever</th>\n",
       "      <th>funny</th>\n",
       "      <th>is</th>\n",
       "      <th>movie</th>\n",
       "      <th>never</th>\n",
       "      <th>this</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   I  SOOOO  This  What  a  best  ever  funny  is  movie  never  this\n",
       "0  0      1     1     0  0     0     0      1   1      1      0     0\n",
       "1  1      0     0     1  1     0     0      0   0      1      1     0\n",
       "2  0      0     0     0  0     1     1      0   0      2      0     1"
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uni_gram(mini_dataset2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "id": "handy-while",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This movie is SOOOO funny!!!',\n",
       " 'What a movie! I never',\n",
       " 'best movie ever!!!!! this movie']"
      ]
     },
     "execution_count": 424,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mini_dataset2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "orange-compilation",
   "metadata": {},
   "source": [
    "### bi-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "original-starter",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bi_gram(X):\n",
    "\n",
    "    \"\"\"\n",
    "    listを受け取りbi-gramのBoWを返す\n",
    "    \"\"\"\n",
    "    \n",
    "    word_list = []\n",
    "    X_lists = []\n",
    "    \n",
    "    for a in range(len(X)):\n",
    "        \n",
    "        X_list = []\n",
    "        X_lists.append(X_list)\n",
    "        \n",
    "        for b in range(len(X[a].split(\" \")) - 1):\n",
    "            \n",
    "            split = \" \".join(X[a].replace(\"!\", \"\").split(\" \")[b : b+2])\n",
    "            split_list = []\n",
    "            split_list.append(split)\n",
    "            word_list.extend(split_list)\n",
    "            X_list.append(split_list)\n",
    "    \n",
    "    word_list = list(set(word_list))\n",
    "    word_list.sort()\n",
    "    sentence_counts = len(X)\n",
    "    out = np.zeros((sentence_counts, len(word_list)))\n",
    "\n",
    "\n",
    "    for j in range(sentence_counts):\n",
    "\n",
    "        for sentence in X_lists[j]:\n",
    "            \n",
    "            sentence = \"\".join(sentence)\n",
    "\n",
    "            for i, word in enumerate(word_list):\n",
    "                \n",
    "                if sentence==word:\n",
    "                    out[j, i] += 1\n",
    "    \n",
    "    out = out.astype(np.int)\n",
    "    df =  pd.DataFrame(out, columns=word_list)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "complex-feeling",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>I never</th>\n",
       "      <th>SOOOO funny</th>\n",
       "      <th>This movie</th>\n",
       "      <th>What a</th>\n",
       "      <th>a movie</th>\n",
       "      <th>best movie</th>\n",
       "      <th>ever this</th>\n",
       "      <th>is SOOOO</th>\n",
       "      <th>movie I</th>\n",
       "      <th>movie ever</th>\n",
       "      <th>movie is</th>\n",
       "      <th>this movie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   I never  SOOOO funny  This movie  What a  a movie  best movie  ever this  \\\n",
       "0        0            1           1       0        0           0          0   \n",
       "1        1            0           0       1        1           0          0   \n",
       "2        0            0           0       0        0           1          1   \n",
       "\n",
       "   is SOOOO  movie I  movie ever  movie is  this movie  \n",
       "0         1        0           0         1           0  \n",
       "1         0        1           0         0           0  \n",
       "2         0        0           1         0           1  "
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_gram(mini_dataset2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "portable-estonia",
   "metadata": {},
   "source": [
    "### IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "genuine-miller",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n_samples = 25000\n",
    "idf = np.log(n_samples/np.arange(1,n_samples))\n",
    "plt.title(\"IDF\")\n",
    "plt.xlabel(\"df(t)\")\n",
    "plt.ylabel(\"IDF\")\n",
    "plt.plot(idf)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "based-death",
   "metadata": {},
   "source": [
    "### ストップワード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "going-intelligence",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>bad</th>\n",
       "      <th>film</th>\n",
       "      <th>good</th>\n",
       "      <th>movie</th>\n",
       "      <th>this</th>\n",
       "      <th>very</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a  bad  film  good  movie  this  very\n",
       "0  0    0     0     1      1     1     1\n",
       "1  1    0     1     1      0     1     0\n",
       "2  0    2     0     0      0     0     3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 頻出トークンを取り除く\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words=[\"is\"], token_pattern=r'\\b\\w+\\b')\n",
    "bow_train = (vectorizer.fit_transform(mini_dataset)).toarray()\n",
    "df = pd.DataFrame(bow_train, columns=vectorizer.get_feature_names())\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "other-example",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.5.zip (1.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.4 MB 4.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting click\n",
      "  Using cached click-7.1.2-py2.py3-none-any.whl (82 kB)\n",
      "Requirement already satisfied: joblib in /Users/teruitakahiro/opt/anaconda3/envs/data_science/lib/python3.7/site-packages (from nltk) (0.17.0)\n",
      "Collecting regex\n",
      "  Downloading regex-2021.4.4-cp37-cp37m-macosx_10_9_x86_64.whl (285 kB)\n",
      "\u001b[K     |████████████████████████████████| 285 kB 13.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /Users/teruitakahiro/opt/anaconda3/envs/data_science/lib/python3.7/site-packages (from nltk) (4.59.0)\n",
      "Building wheels for collected packages: nltk\n",
      "  Building wheel for nltk (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for nltk: filename=nltk-3.5-py3-none-any.whl size=1434671 sha256=46246a24cbeadebd4eec3cbd94594ea6544fb8e36788be361707e3f1a9d51db6\n",
      "  Stored in directory: /Users/teruitakahiro/Library/Caches/pip/wheels/45/6c/46/a1865e7ba706b3817f5d1b2ff7ce8996aabdd0d03d47ba0266\n",
      "Successfully built nltk\n",
      "Installing collected packages: click, regex, nltk\n",
      "Successfully installed click-7.1.2 nltk-3.5 regex-2021.4.4\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "equipped-support",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/teruitakahiro/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stop word : ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "stop_words = nltk.download('stopwords')\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "print(\"stop word : {}\".format(stop_words)) # 'i', 'me', 'my', ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "pregnant-recycling",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bad</th>\n",
       "      <th>good</th>\n",
       "      <th>is</th>\n",
       "      <th>this</th>\n",
       "      <th>very</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bad  good  is  this  very\n",
       "0    0     1   1     1     1\n",
       "1    0     1   1     1     0\n",
       "2    2     0   0     0     3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 次元の呪い回避のために、登場回数の少ないトークンを切る\n",
    "\n",
    "vectorizer = CountVectorizer(token_pattern=r'\\b\\w+\\b', max_features = 5)\n",
    "bow_train = (vectorizer.fit_transform(mini_dataset)).toarray()\n",
    "df = pd.DataFrame(bow_train, columns=vectorizer.get_feature_names())\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quick-genome",
   "metadata": {},
   "source": [
    "## 【問題2】TF-IDFの計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "compatible-burning",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "nominated-amino",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, 25000)"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x_train), len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "romance-touch",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "                                                  stop_words=stop_words,\n",
    "                                                  #oken_pattern=r”(?u)\\b\\w\\w+\\b”,\n",
    "                                                  max_features=5000,\n",
    "                                                  norm=None,\n",
    "                                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "aquatic-truck",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_vec = vectorizer.fit_transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "initial-crisis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00', '000', '10', '100', '11', '12', '13', '13th', '14', '15', '16', '17', '18', '1930', '1930s', '1933', '1940', '1950', '1950s', '1960', '1960s', '1968', '1970', '1970s', '1971', '1972', '1973', '1980', '1980s', '1983', '1984', '1987', '1990', '1993', '1995', '1996', '1997', '1999', '1st', '20', '2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '20th', '24', '25', '2nd', '30', '3000', '30s', '35', '3d', '3rd', '40', '45', '50', '50s', '60', '60s', '70', '70s', '80', '80s', '90', '90s', '99', 'abandoned', 'abc', 'abilities', 'ability', 'able', 'abraham', 'absence', 'absent', 'absolute', 'absolutely', 'absurd', 'abuse', 'abusive', 'abysmal', 'academy', 'accent', 'accents', 'accept', 'acceptable', 'accepted', 'access', 'accident', 'accidentally', 'accompanied', 'accomplished', 'according', 'account', 'accuracy', 'accurate', 'accused', 'achieve', 'achieved', 'achievement', 'acid', 'across', 'act', 'acted', 'acting', 'action', 'actions', 'activities', 'actor', 'actors', 'actress', 'actresses', 'acts', 'actual', 'actually', 'ad', 'adam', 'adams', 'adaptation', 'adaptations', 'adapted', 'add', 'added', 'adding', 'addition', 'adds', 'adequate', 'admire', 'admit', 'admittedly', 'adorable', 'adult', 'adults', 'advance', 'advanced', 'advantage', 'adventure', 'adventures', 'advertising', 'advice', 'advise', 'affair', 'affect', 'affected', 'afford', 'aforementioned', 'afraid', 'africa', 'african', 'afternoon', 'afterwards', 'age', 'aged', 'agent', 'agents', 'ages', 'aging', 'ago', 'agree', 'agreed', 'agrees', 'ah', 'ahead', 'aid', 'aids', 'aimed', 'air', 'aired', 'airplane', 'airport', 'aka', 'akshay', 'al', 'alan', 'alas', 'albeit', 'albert', 'album', 'alcohol', 'alcoholic', 'alec', 'alert', 'alex', 'alexander', 'alfred', 'alice', 'alien', 'aliens', 'alike', 'alison', 'alive', 'allen', 'allow', 'allowed', 'allowing', 'allows', 'almost', 'alone', 'along', 'alongside', 'already', 'alright', 'also', 'alternate', 'although', 'altman', 'altogether', 'always', 'amanda', 'amateur', 'amateurish', 'amazed', 'amazing', 'amazingly', 'ambiguous', 'ambitious', 'america', 'american', 'americans', 'amitabh', 'among', 'amongst', 'amount', 'amounts', 'amusing', 'amy', 'analysis', 'ancient', 'anderson', 'andrew', 'andrews', 'andy', 'angel', 'angela', 'angeles', 'angels', 'anger', 'angle', 'angles', 'angry', 'animal', 'animals', 'animated', 'animation', 'anime', 'ann', 'anna', 'anne', 'annie', 'annoyed', 'annoying', 'another', 'answer', 'answers', 'anthony', 'anti', 'antics', 'antonioni', 'antwone', 'anybody', 'anymore', 'anyone', 'anything', 'anyway', 'anyways', 'anywhere', 'apart', 'apartment', 'ape', 'apes', 'appalling', 'apparent', 'apparently', 'appeal', 'appealing', 'appear', 'appearance', 'appearances', 'appeared', 'appearing', 'appears', 'appreciate', 'appreciated', 'appreciation', 'approach', 'appropriate', 'april', 'area', 'areas', 'arguably', 'argue', 'argument', 'arm', 'armed', 'arms', 'army', 'arnold', 'around', 'arrested', 'arrival', 'arrive', 'arrived', 'arrives', 'arrogant', 'art', 'arthur', 'artificial', 'artist', 'artistic', 'artists', 'arts', 'ashamed', 'ashley', 'asian', 'aside', 'ask', 'asked', 'asking', 'asks', 'asleep', 'aspect', 'aspects', 'ass', 'assassin', 'assault', 'assigned', 'assistant', 'associated', 'assume', 'assumed', 'astaire', 'atlantis', 'atmosphere', 'atmospheric', 'atrocious', 'attached', 'attack', 'attacked', 'attacks', 'attempt', 'attempted', 'attempting', 'attempts', 'attend', 'attention', 'attitude', 'attitudes', 'attorney', 'attracted', 'attraction', 'attractive', 'audience', 'audiences', 'audio', 'aunt', 'austen', 'austin', 'australia', 'australian', 'authentic', 'author', 'authority', 'available', 'average', 'avoid', 'avoided', 'awake', 'award', 'awards', 'aware', 'away', 'awe', 'awesome', 'awful', 'awfully', 'awkward', 'babe', 'baby', 'bacall', 'back', 'backdrop', 'background', 'backgrounds', 'bad', 'badly', 'bag', 'baker', 'bakshi', 'balance', 'baldwin', 'ball', 'ballet', 'balls', 'band', 'bands', 'bang', 'bank', 'banned', 'bar', 'barbara', 'bare', 'barely', 'bargain', 'barry', 'barrymore', 'base', 'baseball', 'based', 'basement', 'basic', 'basically', 'basis', 'basketball', 'bat', 'bath', 'bathroom', 'batman', 'battle', 'battles', 'bay', 'bbc', 'beach', 'bear', 'bears', 'beast', 'beat', 'beaten', 'beating', 'beats', 'beatty', 'beautiful', 'beautifully', 'beauty', 'became', 'become', 'becomes', 'becoming', 'bed', 'bedroom', 'beer', 'began', 'begin', 'beginning', 'begins', 'behave', 'behavior', 'behind', 'beings', 'bela', 'belief', 'beliefs', 'believable', 'believe', 'believed', 'believes', 'believing', 'bell', 'belong', 'belongs', 'beloved', 'belushi', 'ben', 'beneath', 'benefit', 'bergman', 'berlin', 'besides', 'best', 'bet', 'bette', 'better', 'bettie', 'betty', 'beyond', 'bible', 'big', 'bigger', 'biggest', 'biko', 'bill', 'billy', 'bin', 'biography', 'bird', 'birds', 'birth', 'birthday', 'bit', 'bite', 'bits', 'bitter', 'bizarre', 'black', 'blade', 'blah', 'blair', 'blake', 'blame', 'bland', 'blank', 'blatant', 'bleak', 'blend', 'blew', 'blind', 'blob', 'block', 'blockbuster', 'blond', 'blonde', 'blood', 'bloody', 'blow', 'blowing', 'blown', 'blows', 'blue', 'blues', 'blunt', 'bo', 'board', 'boat', 'bob', 'bobby', 'bodies', 'body', 'bold', 'boll', 'bollywood', 'bomb', 'bond', 'bone', 'bonus', 'book', 'books', 'boom', 'boot', 'border', 'bore', 'bored', 'boredom', 'boring', 'born', 'borrowed', 'boss', 'bother', 'bothered', 'bottle', 'bottom', 'bought', 'bound', 'bourne', 'box', 'boxing', 'boy', 'boyfriend', 'boys', 'br', 'brad', 'brady', 'brain', 'brains', 'branagh', 'brand', 'brando', 'brave', 'brazil', 'break', 'breaking', 'breaks', 'breasts', 'breath', 'breathtaking', 'brenda', 'brian', 'bride', 'bridge', 'brief', 'briefly', 'bright', 'brilliance', 'brilliant', 'brilliantly', 'bring', 'bringing', 'brings', 'britain', 'british', 'broad', 'broadcast', 'broadway', 'broke', 'broken', 'brooklyn', 'brooks', 'brosnan', 'brother', 'brothers', 'brought', 'brown', 'bruce', 'brutal', 'brutality', 'brutally', 'buck', 'bucks', 'bud', 'buddies', 'buddy', 'budget', 'buff', 'buffalo', 'buffs', 'bug', 'bugs', 'build', 'building', 'buildings', 'builds', 'built', 'bull', 'bullet', 'bullets', 'bumbling', 'bunch', 'buried', 'burn', 'burned', 'burning', 'burns', 'burt', 'burton', 'bus', 'bush', 'business', 'businessman', 'buster', 'busy', 'butler', 'butt', 'button', 'buy', 'buying', 'cabin', 'cable', 'cage', 'cagney', 'caine', 'cake', 'caliber', 'california', 'call', 'called', 'calling', 'calls', 'calm', 'came', 'cameo', 'cameos', 'camera', 'cameras', 'cameron', 'camp', 'campbell', 'campy', 'canada', 'canadian', 'candy', 'cannibal', 'cannot', 'cant', 'capable', 'capital', 'captain', 'captivating', 'capture', 'captured', 'captures', 'capturing', 'car', 'card', 'cardboard', 'cards', 'care', 'cared', 'career', 'careers', 'careful', 'carefully', 'carell', 'cares', 'caring', 'carl', 'carla', 'carol', 'carpenter', 'carradine', 'carrey', 'carrie', 'carried', 'carries', 'carry', 'carrying', 'cars', 'carter', 'cartoon', 'cartoons', 'cary', 'case', 'cases', 'cash', 'cassidy', 'cast', 'casting', 'castle', 'cat', 'catch', 'catches', 'catching', 'catchy', 'category', 'catherine', 'catholic', 'cats', 'caught', 'cause', 'caused', 'causes', 'causing', 'cave', 'cd', 'celebrity', 'cell', 'celluloid', 'center', 'centered', 'centers', 'central', 'century', 'certain', 'certainly', 'cg', 'cgi', 'chain', 'chair', 'challenge', 'challenging', 'championship', 'chan', 'chance', 'chances', 'change', 'changed', 'changes', 'changing', 'channel', 'channels', 'chaos', 'chaplin', 'chapter', 'character', 'characterization', 'characters', 'charge', 'charisma', 'charismatic', 'charles', 'charlie', 'charlotte', 'charm', 'charming', 'chase', 'chased', 'chases', 'chasing', 'che', 'cheap', 'cheated', 'cheating', 'check', 'checked', 'checking', 'cheek', 'cheese', 'cheesy', 'chemistry', 'chess', 'chest', 'chicago', 'chick', 'chicken', 'chicks', 'chief', 'child', 'childhood', 'childish', 'children', 'chilling', 'china', 'chinese', 'choice', 'choices', 'choose', 'chooses', 'choreographed', 'choreography', 'chorus', 'chose', 'chosen', 'chris', 'christ', 'christian', 'christians', 'christmas', 'christopher', 'christy', 'chuck', 'church', 'cia', 'cinderella', 'cinema', 'cinematic', 'cinematographer', 'cinematography', 'circle', 'circumstances', 'cities', 'citizen', 'city', 'civil', 'civilization', 'claim', 'claimed', 'claims', 'claire', 'clark', 'class', 'classes', 'classic', 'classical', 'classics', 'claus', 'clean', 'clear', 'clearly', 'clever', 'cleverly', 'cliche', 'cliché', 'clichéd', 'clichés', 'cliff', 'climactic', 'climax', 'clint', 'clip', 'clips', 'clock', 'close', 'closed', 'closely', 'closer', 'closest', 'closet', 'closing', 'clothes', 'clothing', 'clown', 'club', 'clue', 'clues', 'clumsy', 'co', 'coach', 'coast', 'code', 'coffee', 'coherent', 'cold', 'cole', 'collection', 'college', 'colonel', 'color', 'colorful', 'colors', 'colour', 'columbo', 'com', 'combat', 'combination', 'combine', 'combined', 'come', 'comedian', 'comedic', 'comedies', 'comedy', 'comes', 'comfort', 'comfortable', 'comic', 'comical', 'comics', 'coming', 'command', 'comment', 'commentary', 'commented', 'comments', 'commercial', 'commercials', 'commit', 'committed', 'common', 'communist', 'community', 'companies', 'companion', 'company', 'compare', 'compared', 'comparing', 'comparison', 'compassion', 'compelled', 'compelling', 'competent', 'competition', 'complain', 'complaint', 'complete', 'completely', 'complex', 'complexity', 'complicated', 'composed', 'composer', 'computer', 'con', 'conceived', 'concept', 'concern', 'concerned', 'concerning', 'concerns', 'concert', 'conclusion', 'condition', 'conditions', 'confess', 'confidence', 'conflict', 'conflicts', 'confused', 'confusing', 'confusion', 'connect', 'connected', 'connection', 'connery', 'consequences', 'conservative', 'consider', 'considerable', 'considered', 'considering', 'consistent', 'consistently', 'consists', 'conspiracy', 'constant', 'constantly', 'constructed', 'construction', 'contact', 'contain', 'contained', 'contains', 'contemporary', 'content', 'contest', 'context', 'continue', 'continued', 'continues', 'continuity', 'contract', 'contrary', 'contrast', 'contrived', 'control', 'controversial', 'conventional', 'conversation', 'conversations', 'convey', 'convince', 'convinced', 'convincing', 'convincingly', 'convoluted', 'cook', 'cool', 'cooper', 'cop', 'copies', 'cops', 'copy', 'core', 'corner', 'corny', 'corporate', 'corpse', 'correct', 'correctly', 'corrupt', 'corruption', 'cost', 'costs', 'costume', 'costumes', 'could', 'count', 'counter', 'countless', 'countries', 'country', 'countryside', 'couple', 'couples', 'courage', 'course', 'court', 'cousin', 'cover', 'covered', 'covers', 'cowboy', 'cox', 'crack', 'craft', 'crafted', 'craig', 'crap', 'crappy', 'crash', 'craven', 'crawford', 'crazy', 'create', 'created', 'creates', 'creating', 'creation', 'creative', 'creativity', 'creator', 'creators', 'creature', 'creatures', 'credibility', 'credible', 'credit', 'credits', 'creep', 'creepy', 'crew', 'cried', 'crime', 'crimes', 'criminal', 'criminals', 'cringe', 'crisis', 'critic', 'critical', 'criticism', 'critics', 'crocodile', 'cross', 'crowd', 'crucial', 'crude', 'cruel', 'cruise', 'crush', 'cry', 'crying', 'crystal', 'cuba', 'cube', 'cult', 'cultural', 'culture', 'cup', 'cure', 'curiosity', 'curious', 'current', 'currently', 'curse', 'curtis', 'cusack', 'cut', 'cute', 'cuts', 'cutting', 'cynical', 'dad', 'daddy', 'daily', 'dalton', 'damage', 'damme', 'damn', 'damon', 'dan', 'dana', 'dance', 'dancer', 'dancers', 'dances', 'dancing', 'danes', 'danger', 'dangerous', 'daniel', 'danny', 'dare', 'daring', 'dark', 'darker', 'darkness', 'darren', 'date', 'dated', 'dating', 'daughter', 'daughters', 'dave', 'david', 'davies', 'davis', 'dawn', 'dawson', 'day', 'days', 'de', 'dead', 'deadly', 'deaf', 'deal', 'dealing', 'deals', 'dealt', 'dean', 'dear', 'death', 'deaths', 'debut', 'decade', 'decades', 'deceased', 'decent', 'decide', 'decided', 'decides', 'decision', 'decisions', 'dedicated', 'dee', 'deep', 'deeper', 'deeply', 'defeat', 'defend', 'defense', 'defined', 'definite', 'definitely', 'definition', 'degree', 'del', 'deliberately', 'delight', 'delightful', 'deliver', 'delivered', 'delivering', 'delivers', 'delivery', 'demand', 'demands', 'demented', 'demon', 'demons', 'deniro', 'dennis', 'dentist', 'denzel', 'department', 'depicted', 'depicting', 'depiction', 'depicts', 'depressed', 'depressing', 'depression', 'depth', 'der', 'derek', 'descent', 'describe', 'described', 'describes', 'description', 'desert', 'deserve', 'deserved', 'deserves', 'design', 'designed', 'designs', 'desire', 'desired', 'despair', 'desperate', 'desperately', 'desperation', 'despite', 'destiny', 'destroy', 'destroyed', 'destroying', 'destruction', 'detail', 'detailed', 'details', 'detective', 'determined', 'develop', 'developed', 'developing', 'development', 'develops', 'device', 'devil', 'devoid', 'devoted', 'dialog', 'dialogs', 'dialogue', 'dialogues', 'diamond', 'diana', 'diane', 'dick', 'dickens', 'die', 'died', 'dies', 'difference', 'differences', 'different', 'difficult', 'dig', 'digital', 'dignity', 'dimension', 'dimensional', 'din', 'dinner', 'dinosaur', 'dinosaurs', 'dire', 'direct', 'directed', 'directing', 'direction', 'directions', 'directly', 'director', 'directorial', 'directors', 'directs', 'dirty', 'disagree', 'disappear', 'disappeared', 'disappoint', 'disappointed', 'disappointing', 'disappointment', 'disaster', 'disbelief', 'disc', 'discover', 'discovered', 'discovers', 'discovery', 'discuss', 'discussion', 'disease', 'disgusting', 'disjointed', 'dislike', 'disney', 'display', 'displayed', 'displays', 'distance', 'distant', 'distinct', 'distracting', 'distribution', 'disturbed', 'disturbing', 'divorce', 'dixon', 'doc', 'doctor', 'documentaries', 'documentary', 'dog', 'dogs', 'doll', 'dollar', 'dollars', 'dolls', 'dolph', 'domestic', 'domino', 'donald', 'done', 'donna', 'doo', 'doom', 'doomed', 'door', 'doors', 'dorothy', 'double', 'doubt', 'doubts', 'douglas', 'downhill', 'downright', 'dozen', 'dozens', 'dr', 'dracula', 'drag', 'dragged', 'dragon', 'drags', 'drake', 'drama', 'dramas', 'dramatic', 'draw', 'drawing', 'drawn', 'draws', 'dreadful', 'dream', 'dreams', 'dreary', 'dreck', 'dress', 'dressed', 'dressing', 'drew', 'drink', 'drinking', 'drive', 'drivel', 'driven', 'driver', 'drives', 'driving', 'drop', 'dropped', 'dropping', 'drops', 'drug', 'drugs', 'drunk', 'drunken', 'dry', 'dub', 'dubbed', 'dubbing', 'dud', 'dude', 'due', 'duke', 'dull', 'dumb', 'duo', 'dust', 'dutch', 'duty', 'dvd', 'dying', 'dynamic', 'eager', 'ear', 'earl', 'earlier', 'early', 'earned', 'ears', 'earth', 'ease', 'easier', 'easily', 'east', 'eastern', 'eastwood', 'easy', 'eat', 'eaten', 'eating', 'eccentric', 'ed', 'eddie', 'edgar', 'edge', 'edgy', 'edie', 'edited', 'editing', 'edition', 'editor', 'education', 'educational', 'edward', 'eerie', 'effect', 'effective', 'effectively', 'effects', 'effort', 'efforts', 'ego', 'eight', 'eighties', 'either', 'elaborate', 'elderly', 'elegant', 'element', 'elements', 'elephant', 'elizabeth', 'ellen', 'elm', 'else', 'elsewhere', 'elvira', 'elvis', 'em', 'embarrassed', 'embarrassing', 'embarrassment', 'emily', 'emma', 'emotion', 'emotional', 'emotionally', 'emotions', 'empathy', 'emperor', 'emphasis', 'empire', 'empty', 'encounter', 'encounters', 'end', 'endearing', 'ended', 'ending', 'endings', 'endless', 'ends', 'endure', 'enemies', 'enemy', 'energy', 'engage', 'engaged', 'engaging', 'england', 'english', 'enjoy', 'enjoyable', 'enjoyed', 'enjoying', 'enjoyment', 'enjoys', 'enormous', 'enough', 'ensemble', 'ensues', 'enter', 'enterprise', 'enters', 'entertain', 'entertained', 'entertaining', 'entertainment', 'enthusiasm', 'entire', 'entirely', 'entry', 'environment', 'epic', 'episode', 'episodes', 'equal', 'equally', 'equipment', 'equivalent', 'era', 'eric', 'erotic', 'errors', 'escape', 'escaped', 'escapes', 'especially', 'essence', 'essential', 'essentially', 'established', 'estate', 'et', 'etc', 'ethan', 'eugene', 'europe', 'european', 'eva', 'eve', 'even', 'evening', 'event', 'events', 'eventually', 'ever', 'every', 'everybody', 'everyday', 'everyone', 'everything', 'everywhere', 'evidence', 'evident', 'evil', 'ex', 'exact', 'exactly', 'exaggerated', 'example', 'examples', 'excellent', 'except', 'exception', 'exceptional', 'exceptionally', 'excessive', 'exchange', 'excited', 'excitement', 'exciting', 'excuse', 'executed', 'execution', 'executive', 'exercise', 'exist', 'existed', 'existence', 'existent', 'exists', 'exotic', 'expect', 'expectations', 'expected', 'expecting', 'expedition', 'expensive', 'experience', 'experienced', 'experiences', 'experiment', 'experiments', 'expert', 'explain', 'explained', 'explaining', 'explains', 'explanation', 'explicit', 'exploitation', 'exploration', 'explore', 'explored', 'explosion', 'explosions', 'exposed', 'exposure', 'express', 'expressed', 'expression', 'expressions', 'extended', 'extent', 'extra', 'extraordinary', 'extras', 'extreme', 'extremely', 'eye', 'eyed', 'eyes', 'eyre', 'fabulous', 'face', 'faced', 'faces', 'facial', 'facing', 'fact', 'factor', 'factory', 'facts', 'fail', 'failed', 'failing', 'fails', 'failure', 'fair', 'fairly', 'fairy', 'faith', 'faithful', 'fake', 'falk', 'fall', 'fallen', 'falling', 'falls', 'false', 'fame', 'familiar', 'families', 'family', 'famous', 'fan', 'fancy', 'fans', 'fantastic', 'fantasy', 'far', 'farce', 'fare', 'farm', 'farrell', 'fascinated', 'fascinating', 'fashion', 'fashioned', 'fast', 'faster', 'fat', 'fatal', 'fate', 'father', 'fault', 'faults', 'favor', 'favorite', 'favorites', 'favourite', 'fay', 'fbi', 'fear', 'fears', 'feature', 'featured', 'features', 'featuring', 'fed', 'feed', 'feel', 'feeling', 'feelings', 'feels', 'feet', 'felix', 'fell', 'fellow', 'felt', 'female', 'feminist', 'femme', 'fest', 'festival', 'fetched', 'fever', 'fi', 'fiancé', 'fiction', 'fictional', 'fido', 'field', 'fifteen', 'fight', 'fighter', 'fighting', 'fights', 'figure', 'figured', 'figures', 'files', 'fill', 'filled', 'film', 'filmed', 'filming', 'filmmaker', 'filmmakers', 'films', 'final', 'finale', 'finally', 'financial', 'find', 'finding', 'finds', 'fine', 'finest', 'finger', 'finish', 'finished', 'fire', 'fired', 'firm', 'first', 'firstly', 'fish', 'fisher', 'fit', 'fits', 'fitting', 'five', 'fix', 'flash', 'flashback', 'flashbacks', 'flat', 'flaw', 'flawed', 'flawless', 'flaws', 'flesh', 'flick', 'flicks', 'flies', 'flight', 'floating', 'floor', 'flop', 'florida', 'flow', 'fly', 'flying', 'flynn', 'focus', 'focused', 'focuses', 'focusing', 'folk', 'folks', 'follow', 'followed', 'following', 'follows', 'fond', 'fonda', 'food', 'fool', 'fooled', 'foot', 'footage', 'football', 'forbidden', 'force', 'forced', 'forces', 'ford', 'foreign', 'forest', 'forever', 'forget', 'forgettable', 'forgive', 'forgot', 'forgotten', 'form', 'format', 'former', 'forms', 'formula', 'formulaic', 'forth', 'fortunately', 'fortune', 'forty', 'forward', 'foster', 'fought', 'foul', 'found', 'four', 'fourth', 'fox', 'frame', 'france', 'franchise', 'francis', 'francisco', 'franco', 'frank', 'frankenstein', 'frankie', 'frankly', 'freak', 'fred', 'freddy', 'free', 'freedom', 'freeman', 'french', 'frequent', 'frequently', 'fresh', 'friday', 'friend', 'friendly', 'friends', 'friendship', 'frightening', 'front', 'frustrated', 'frustration', 'fu', 'fulci', 'full', 'fuller', 'fully', 'fun', 'funeral', 'funnier', 'funniest', 'funny', 'furious', 'furthermore', 'fury', 'future', 'futuristic', 'fx', 'gabriel', 'gadget', 'gag', 'gags', 'gain', 'game', 'games', 'gandhi', 'gang', 'gangster', 'gangsters', 'garbage', 'garbo', 'garden', 'gary', 'gas', 'gave', 'gay', 'gem', 'gender', 'gene', 'general', 'generally', 'generated', 'generation', 'generations', 'generic', 'generous', 'genius', 'genre', 'genres', 'gentle', 'gentleman', 'genuine', 'genuinely', 'george', 'gerard', 'german', 'germans', 'germany', 'get', 'gets', 'getting', 'ghost', 'ghosts', 'giallo', 'giant', 'gift', 'gifted', 'ginger', 'girl', 'girlfriend', 'girls', 'give', 'given', 'gives', 'giving', 'glad', 'glass', 'glasses', 'glenn', 'glimpse', 'global', 'glorious', 'glory', 'glover', 'go', 'goal', 'god', 'godfather', 'godzilla', 'goes', 'going', 'gold', 'goldberg', 'golden', 'gone', 'gonna', 'good', 'goodness', 'goofy', 'gordon', 'gore', 'gorgeous', 'gory', 'got', 'gothic', 'gotta', 'gotten', 'government', 'grab', 'grabs', 'grace', 'grade', 'gradually', 'graham', 'grand', 'grandfather', 'grandmother', 'grant', 'granted', 'graphic', 'graphics', 'grasp', 'gratuitous', 'grave', 'gray', 'grayson', 'great', 'greater', 'greatest', 'greatly', 'greatness', 'greed', 'greedy', 'greek', 'green', 'greg', 'grew', 'grey', 'griffith', 'grim', 'grinch', 'gripping', 'gritty', 'gross', 'ground', 'group', 'groups', 'grow', 'growing', 'grown', 'grows', 'gruesome', 'guarantee', 'guard', 'guess', 'guessed', 'guessing', 'guest', 'guide', 'guilt', 'guilty', 'gun', 'gundam', 'guns', 'guts', 'guy', 'guys', 'ha', 'hair', 'hal', 'half', 'halfway', 'hall', 'halloween', 'ham', 'hamilton', 'hamlet', 'hammer', 'hand', 'handed', 'handful', 'handle', 'handled', 'hands', 'handsome', 'hang', 'hanging', 'hank', 'hanks', 'happen', 'happened', 'happening', 'happens', 'happily', 'happiness', 'happy', 'hard', 'hardcore', 'harder', 'hardly', 'hardy', 'harm', 'harris', 'harry', 'harsh', 'hart', 'hartley', 'harvey', 'hat', 'hate', 'hated', 'hates', 'hatred', 'haunted', 'haunting', 'hbo', 'head', 'headed', 'heads', 'health', 'hear', 'heard', 'hearing', 'heart', 'hearted', 'hearts', 'heat', 'heaven', 'heavily', 'heavy', 'heck', 'heights', 'held', 'helen', 'helicopter', 'hell', 'hello', 'help', 'helped', 'helping', 'helps', 'hence', 'henry', 'hero', 'heroes', 'heroic', 'heroine', 'heston', 'hey', 'hidden', 'hide', 'hideous', 'hiding', 'high', 'higher', 'highest', 'highlight', 'highlights', 'highly', 'hilarious', 'hilariously', 'hill', 'hills', 'hint', 'hints', 'hip', 'hippie', 'hire', 'hired', 'historical', 'historically', 'history', 'hit', 'hitchcock', 'hitler', 'hits', 'hitting', 'ho', 'hoffman', 'hold', 'holding', 'holds', 'hole', 'holes', 'holiday', 'hollow', 'holly', 'hollywood', 'holmes', 'holy', 'homage', 'home', 'homeless', 'homer', 'homosexual', 'honest', 'honestly', 'honesty', 'hong', 'honor', 'hood', 'hook', 'hooked', 'hop', 'hope', 'hoped', 'hopefully', 'hopeless', 'hopes', 'hoping', 'hopper', 'horrendous', 'horrible', 'horribly', 'horrid', 'horrific', 'horrifying', 'horror', 'horrors', 'horse', 'horses', 'hospital', 'host', 'hot', 'hotel', 'hour', 'hours', 'house', 'household', 'houses', 'howard', 'however', 'hudson', 'huge', 'hugh', 'huh', 'human', 'humanity', 'humans', 'humble', 'humor', 'humorous', 'humour', 'hundred', 'hundreds', 'hung', 'hunt', 'hunter', 'hunters', 'hunting', 'hurt', 'hurts', 'husband', 'hyde', 'hype', 'hysterical', 'ian', 'ice', 'icon', 'idea', 'ideal', 'ideas', 'identify', 'identity', 'idiot', 'idiotic', 'idiots', 'ignorant', 'ignore', 'ignored', 'ii', 'iii', 'ill', 'illegal', 'illness', 'illogical', 'im', 'image', 'imagery', 'images', 'imagination', 'imaginative', 'imagine', 'imagined', 'imdb', 'imitation', 'immediate', 'immediately', 'immensely', 'impact', 'implausible', 'importance', 'important', 'importantly', 'impossible', 'impress', 'impressed', 'impression', 'impressive', 'improve', 'improved', 'improvement', 'inane', 'inappropriate', 'incident', 'include', 'included', 'includes', 'including', 'incoherent', 'incompetent', 'incomprehensible', 'increasingly', 'incredible', 'incredibly', 'indeed', 'independent', 'india', 'indian', 'indians', 'indie', 'individual', 'individuals', 'inducing', 'indulgent', 'industry', 'inept', 'inevitable', 'inevitably', 'infamous', 'inferior', 'influence', 'influenced', 'information', 'ingredients', 'initial', 'initially', 'inner', 'innocence', 'innocent', 'innovative', 'insane', 'inside', 'insight', 'inspector', 'inspiration', 'inspired', 'inspiring', 'installment', 'instance', 'instant', 'instantly', 'instead', 'instinct', 'insult', 'insulting', 'integrity', 'intellectual', 'intelligence', 'intelligent', 'intended', 'intense', 'intensity', 'intent', 'intention', 'intentionally', 'intentions', 'interaction', 'interest', 'interested', 'interesting', 'interests', 'international', 'internet', 'interpretation', 'interview', 'interviews', 'intimate', 'intrigue', 'intrigued', 'intriguing', 'introduce', 'introduced', 'introduces', 'introduction', 'invasion', 'invented', 'inventive', 'investigate', 'investigation', 'invisible', 'involve', 'involved', 'involvement', 'involves', 'involving', 'iran', 'iraq', 'ireland', 'irish', 'iron', 'ironic', 'ironically', 'irony', 'irrelevant', 'irritating', 'island', 'isolated', 'israel', 'issue', 'issues', 'italian', 'italy', 'jack', 'jackie', 'jackson', 'jail', 'jake', 'james', 'jamie', 'jane', 'japan', 'japanese', 'jason', 'jaw', 'jaws', 'jay', 'jazz', 'jealous', 'jean', 'jeff', 'jeffrey', 'jennifer', 'jenny', 'jeremy', 'jerk', 'jerry', 'jesse', 'jessica', 'jesus', 'jet', 'jewish', 'jim', 'jimmy', 'joan', 'job', 'jobs', 'joe', 'joel', 'joey', 'john', 'johnny', 'johnson', 'join', 'joined', 'joke', 'jokes', 'jon', 'jonathan', 'jones', 'joseph', 'josh', 'journalist', 'journey', 'joy', 'jr', 'judge', 'judging', 'judy', 'julia', 'julian', 'julie', 'jump', 'jumped', 'jumping', 'jumps', 'june', 'jungle', 'junior', 'junk', 'justice', 'justify', 'justin', 'juvenile', 'kane', 'kapoor', 'karen', 'karloff', 'kate', 'kay', 'keaton', 'keep', 'keeping', 'keeps', 'keith', 'kelly', 'ken', 'kennedy', 'kenneth', 'kept', 'kevin', 'key', 'khan', 'kick', 'kicked', 'kicking', 'kicks', 'kid', 'kidding', 'kidnapped', 'kids', 'kill', 'killed', 'killer', 'killers', 'killing', 'killings', 'kills', 'kim', 'kind', 'kinda', 'kinds', 'king', 'kingdom', 'kirk', 'kiss', 'kissing', 'kitchen', 'knew', 'knife', 'knock', 'know', 'knowing', 'knowledge', 'known', 'knows', 'kong', 'korean', 'kubrick', 'kudos', 'kumar', 'kung', 'kurosawa', 'kurt', 'kyle', 'la', 'lab', 'lack', 'lacked', 'lacking', 'lacks', 'ladies', 'lady', 'laid', 'lake', 'lame', 'land', 'landing', 'landscape', 'landscapes', 'lane', 'language', 'large', 'largely', 'larger', 'larry', 'last', 'lasted', 'late', 'lately', 'later', 'latest', 'latin', 'latter', 'laugh', 'laughable', 'laughably', 'laughed', 'laughing', 'laughs', 'laughter', 'laura', 'laurel', 'law', 'lawrence', 'laws', 'lawyer', 'lay', 'lazy', 'le', 'lead', 'leader', 'leading', 'leads', 'league', 'learn', 'learned', 'learning', 'learns', 'least', 'leave', 'leaves', 'leaving', 'led', 'lee', 'left', 'leg', 'legacy', 'legal', 'legend', 'legendary', 'legs', 'lemmon', 'lena', 'length', 'lengthy', 'leo', 'leon', 'leonard', 'les', 'lesbian', 'leslie', 'less', 'lesser', 'lesson', 'lessons', 'let', 'lets', 'letter', 'letters', 'letting', 'level', 'levels', 'lewis', 'li', 'liberal', 'library', 'lie', 'lies', 'life', 'lifestyle', 'lifetime', 'light', 'lighting', 'lights', 'likable', 'like', 'liked', 'likely', 'likes', 'likewise', 'liking', 'lily', 'limited', 'limits', 'lincoln', 'linda', 'line', 'liners', 'lines', 'link', 'lion', 'lips', 'lisa', 'list', 'listed', 'listen', 'listening', 'lit', 'literally', 'literature', 'little', 'live', 'lived', 'lively', 'lives', 'living', 'lloyd', 'load', 'loaded', 'loads', 'local', 'location', 'locations', 'locked', 'logic', 'logical', 'lol', 'london', 'lone', 'lonely', 'long', 'longer', 'look', 'looked', 'looking', 'looks', 'loose', 'loosely', 'lord', 'los', 'lose', 'loser', 'losers', 'loses', 'losing', 'loss', 'lost', 'lot', 'lots', 'lou', 'loud', 'louis', 'louise', 'lousy', 'lovable', 'love', 'loved', 'lovely', 'lover', 'lovers', 'loves', 'loving', 'low', 'lower', 'lowest', 'loyal', 'loyalty', 'lucas', 'luck', 'luckily', 'lucky', 'lucy', 'ludicrous', 'lugosi', 'luke', 'lumet', 'lundgren', 'lust', 'lying', 'lynch', 'lyrics', 'macarthur', 'machine', 'machines', 'macy', 'mad', 'made', 'madness', 'madonna', 'mafia', 'magazine', 'maggie', 'magic', 'magical', 'magnificent', 'maid', 'mail', 'main', 'mainly', 'mainstream', 'maintain', 'major', 'majority', 'make', 'maker', 'makers', 'makes', 'makeup', 'making', 'male', 'mall', 'malone', 'man', 'manage', 'managed', 'manager', 'manages', 'manhattan', 'maniac', 'manipulative', 'mankind', 'mann', 'manner', 'mansion', 'many', 'map', 'march', 'margaret', 'maria', 'marie', 'mario', 'marion', 'mark', 'market', 'marketing', 'marks', 'marriage', 'married', 'marry', 'mars', 'marshall', 'martial', 'martin', 'marty', 'marvelous', 'mary', 'mask', 'mass', 'massacre', 'masses', 'massive', 'master', 'masterful', 'masterpiece', 'masterpieces', 'masters', 'match', 'matched', 'matches', 'mate', 'material', 'matrix', 'matt', 'matter', 'matters', 'matthau', 'matthew', 'mature', 'max', 'may', 'maybe', 'mayor', 'mclaglen', 'mean', 'meaning', 'meaningful', 'meaningless', 'means', 'meant', 'meanwhile', 'measure', 'meat', 'mechanical', 'media', 'medical', 'mediocre', 'medium', 'meet', 'meeting', 'meets', 'mel', 'melodrama', 'melodramatic', 'melting', 'member', 'members', 'memorable', 'memories', 'memory', 'men', 'menace', 'menacing', 'mental', 'mentally', 'mention', 'mentioned', 'mentioning', 'mentions', 'mere', 'merely', 'merit', 'merits', 'meryl', 'mess', 'message', 'messages', 'messed', 'met', 'metal', 'metaphor', 'method', 'methods', 'mexican', 'mexico', 'mgm', 'michael', 'michelle', 'mickey', 'mid', 'middle', 'midnight', 'might', 'mighty', 'miike', 'mike', 'mild', 'mildly', 'mildred', 'mile', 'miles', 'military', 'mill', 'miller', 'million', 'millions', 'mind', 'minded', 'mindless', 'minds', 'mine', 'mini', 'minimal', 'minimum', 'minor', 'minute', 'minutes', 'miracle', 'mirror', 'miscast', 'miserable', 'miserably', 'misery', 'miss', 'missed', 'misses', 'missing', 'mission', 'mistake', 'mistaken', 'mistakes', 'mistress', 'mitchell', 'mix', 'mixed', 'mixture', 'miyazaki', 'mob', 'model', 'models', 'modern', 'modesty', 'molly', 'mom', 'moment', 'moments', 'money', 'monk', 'monkey', 'monkeys', 'monster', 'monsters', 'montage', 'montana', 'month', 'months', 'mood', 'moody', 'moon', 'moore', 'moral', 'morality', 'morgan', 'morning', 'moronic', 'morris', 'mostly', 'mother', 'motion', 'motivation', 'motivations', 'motives', 'mountain', 'mountains', 'mouse', 'mouth', 'move', 'moved', 'movement', 'movements', 'moves', 'movie', 'movies', 'moving', 'mr', 'mrs', 'ms', 'mst3k', 'mtv', 'much', 'multi', 'multiple', 'mummy', 'mundane', 'murder', 'murdered', 'murderer', 'murderous', 'murders', 'murphy', 'murray', 'museum', 'music', 'musical', 'musicals', 'muslim', 'must', 'myers', 'mysteries', 'mysterious', 'mystery', 'nail', 'naive', 'naked', 'name', 'named', 'namely', 'names', 'nancy', 'narration', 'narrative', 'narrator', 'nasty', 'nathan', 'nation', 'national', 'native', 'natural', 'naturally', 'nature', 'navy', 'nazi', 'nazis', 'near', 'nearby', 'nearly', 'neat', 'necessarily', 'necessary', 'neck', 'ned', 'need', 'needed', 'needless', 'needs', 'negative', 'neighbor', 'neighborhood', 'neighbors', 'neil', 'neither', 'nelson', 'neo', 'nephew', 'nerd', 'nervous', 'network', 'never', 'nevertheless', 'new', 'newly', 'news', 'newspaper', 'next', 'nice', 'nicely', 'nicholas', 'nicholson', 'nick', 'nicole', 'night', 'nightmare', 'nightmares', 'nights', 'nine', 'ninja', 'niro', 'noble', 'nobody', 'noir', 'noise', 'nominated', 'nomination', 'non', 'none', 'nonetheless', 'nonsense', 'nonsensical', 'normal', 'normally', 'norman', 'north', 'nose', 'nostalgia', 'nostalgic', 'notable', 'notably', 'notch', 'note', 'noted', 'notes', 'nothing', 'notice', 'noticed', 'notion', 'notorious', 'novak', 'novel', 'novels', 'nowadays', 'nowhere', 'nuclear', 'nude', 'nudity', 'number', 'numbers', 'numerous', 'nurse', 'nuts', 'nyc', 'object', 'obnoxious', 'obscure', 'obsessed', 'obsession', 'obvious', 'obviously', 'occasion', 'occasional', 'occasionally', 'occur', 'occurred', 'occurs', 'ocean', 'odd', 'oddly', 'odds', 'offended', 'offensive', 'offer', 'offered', 'offering', 'offers', 'office', 'officer', 'officers', 'official', 'often', 'oh', 'oil', 'ok', 'okay', 'old', 'older', 'oliver', 'olivier', 'ollie', 'omen', 'one', 'ones', 'online', 'onto', 'open', 'opened', 'opening', 'opens', 'opera', 'operation', 'opinion', 'opinions', 'opportunities', 'opportunity', 'opposed', 'opposite', 'orange', 'order', 'orders', 'ordinary', 'original', 'originality', 'originally', 'orleans', 'orson', 'oscar', 'oscars', 'othello', 'others', 'otherwise', 'ought', 'outcome', 'outer', 'outfit', 'outrageous', 'outside', 'outstanding', 'overacting', 'overall', 'overcome', 'overdone', 'overlook', 'overlooked', 'overly', 'overrated', 'overwhelming', 'owen', 'owner', 'oz', 'pace', 'paced', 'pacing', 'pacino', 'pack', 'package', 'packed', 'page', 'paid', 'pain', 'painful', 'painfully', 'paint', 'painted', 'painting', 'pair', 'pal', 'palace', 'palma', 'paltrow', 'pamela', 'pan', 'panic', 'pants', 'paper', 'par', 'parallel', 'paranoia', 'parent', 'parents', 'paris', 'park', 'parker', 'parody', 'part', 'particular', 'particularly', 'parties', 'partly', 'partner', 'parts', 'party', 'pass', 'passable', 'passed', 'passes', 'passing', 'passion', 'passionate', 'past', 'pat', 'path', 'pathetic', 'patience', 'patient', 'patients', 'patrick', 'paul', 'paulie', 'pay', 'paying', 'pays', 'peace', 'peak', 'pearl', 'people', 'peoples', 'per', 'perfect', 'perfection', 'perfectly', 'perform', 'performance', 'performances', 'performed', 'performer', 'performers', 'performing', 'performs', 'perhaps', 'period', 'perry', 'person', 'persona', 'personal', 'personalities', 'personality', 'personally', 'persons', 'perspective', 'pet', 'pete', 'peter', 'peters', 'petty', 'pg', 'phantom', 'phil', 'philip', 'philosophical', 'philosophy', 'phone', 'phony', 'photo', 'photographed', 'photographer', 'photography', 'photos', 'physical', 'physically', 'piano', 'pick', 'picked', 'picking', 'picks', 'picture', 'pictures', 'pie', 'piece', 'pieces', 'pierce', 'pig', 'pile', 'pilot', 'pin', 'pink', 'pit', 'pitch', 'pitt', 'pity', 'place', 'placed', 'places', 'plague', 'plain', 'plan', 'plane', 'planet', 'planned', 'planning', 'plans', 'plant', 'plastic', 'plausible', 'play', 'played', 'player', 'players', 'playing', 'plays', 'pleasant', 'pleasantly', 'please', 'pleased', 'pleasure', 'plenty', 'plight', 'plot', 'plots', 'plus', 'poem', 'poetic', 'poetry', 'poignant', 'point', 'pointed', 'pointless', 'points', 'pokemon', 'polanski', 'police', 'polished', 'political', 'politically', 'politics', 'pool', 'poor', 'poorly', 'pop', 'popcorn', 'pops', 'popular', 'popularity', 'population', 'porn', 'porno', 'portion', 'portrait', 'portray', 'portrayal', 'portrayed', 'portraying', 'portrays', 'position', 'positive', 'possessed', 'possibilities', 'possibility', 'possible', 'possibly', 'post', 'poster', 'pot', 'potential', 'potentially', 'poverty', 'powell', 'power', 'powerful', 'powers', 'practically', 'practice', 'praise', 'pre', 'precious', 'predictable', 'prefer', 'pregnant', 'premise', 'prepared', 'prequel', 'presence', 'present', 'presentation', 'presented', 'presents', 'president', 'press', 'pressure', 'preston', 'presumably', 'pretend', 'pretending', 'pretentious', 'pretty', 'prevent', 'preview', 'previous', 'previously', 'price', 'priceless', 'pride', 'priest', 'primarily', 'primary', 'prime', 'prince', 'princess', 'principal', 'print', 'prior', 'prison', 'prisoner', 'prisoners', 'private', 'prize', 'pro', 'probably', 'problem', 'problems', 'proceedings', 'proceeds', 'process', 'produce', 'produced', 'producer', 'producers', 'producing', 'product', 'production', 'productions', 'professional', 'professor', 'profound', 'program', 'progress', 'progresses', 'project', 'projects', 'prom', 'promise', 'promised', 'promises', 'promising', 'proof', 'propaganda', 'proper', 'properly', 'property', 'props', 'prostitute', 'protagonist', 'protagonists', 'protect', 'proud', 'prove', 'proved', 'proves', 'provide', 'provided', 'provides', 'providing', 'provoking', 'pseudo', 'psychiatrist', 'psychic', 'psycho', 'psychological', 'psychotic', 'public', 'pull', 'pulled', 'pulling', 'pulls', 'pulp', 'punch', 'punishment', 'punk', 'puppet', 'purchase', 'purchased', 'pure', 'purely', 'purple', 'purpose', 'purposes', 'pursuit', 'push', 'pushed', 'pushing', 'put', 'puts', 'putting', 'qualities', 'quality', 'queen', 'quest', 'question', 'questionable', 'questions', 'quick', 'quickly', 'quiet', 'quinn', 'quirky', 'quit', 'quite', 'quote', 'quotes', 'rabbit', 'race', 'rachel', 'racial', 'racism', 'racist', 'radio', 'rage', 'rain', 'raise', 'raised', 'raising', 'ralph', 'rambo', 'ramones', 'ran', 'random', 'randomly', 'randy', 'range', 'rangers', 'rank', 'ranks', 'rap', 'rape', 'raped', 'rare', 'rarely', 'rat', 'rate', 'rated', 'rather', 'rating', 'ratings', 'rats', 'rave', 'raw', 'ray', 'raymond', 'reach', 'reached', 'reaches', 'reaching', 'react', 'reaction', 'reactions', 'read', 'reader', 'reading', 'reads', 'ready', 'real', 'realise', 'realism', 'realistic', 'reality', 'realize', 'realized', 'realizes', 'realizing', 'really', 'reason', 'reasonable', 'reasonably', 'reasons', 'rebel', 'recall', 'receive', 'received', 'receives', 'recent', 'recently', 'recognition', 'recognize', 'recognized', 'recommend', 'recommended', 'record', 'recorded', 'recording', 'red', 'redeeming', 'redemption', 'reduced', 'reed', 'reel', 'reference', 'references', 'reflect', 'reflection', 'refreshing', 'refuses', 'regard', 'regarding', 'regardless', 'regret', 'regular', 'reid', 'relate', 'related', 'relation', 'relations', 'relationship', 'relationships', 'relative', 'relatively', 'relatives', 'release', 'released', 'relevant', 'relief', 'relies', 'religion', 'religious', 'remain', 'remaining', 'remains', 'remake', 'remarkable', 'remarkably', 'remarks', 'remember', 'remembered', 'remind', 'reminded', 'reminds', 'reminiscent', 'remote', 'remotely', 'removed', 'rendition', 'rent', 'rental', 'rented', 'renting', 'repeat', 'repeated', 'repeatedly', 'repetitive', 'replaced', 'report', 'reporter', 'represent', 'represented', 'represents', 'reputation', 'required', 'requires', 'rescue', 'research', 'resemblance', 'resemble', 'resembles', 'resident', 'resist', 'resolution', 'resort', 'resources', 'respect', 'respected', 'response', 'responsibility', 'responsible', 'rest', 'restaurant', 'restored', 'result', 'resulting', 'results', 'retarded', 'retired', 'return', 'returned', 'returning', 'returns', 'reunion', 'reveal', 'revealed', 'revealing', 'reveals', 'revelation', 'revenge', 'review', 'reviewer', 'reviewers', 'reviews', 'revolution', 'revolutionary', 'revolves', 'rex', 'reynolds', 'rich', 'richard', 'richards', 'richardson', 'rick', 'rid', 'ridden', 'ride', 'ridiculous', 'ridiculously', 'riding', 'right', 'rights', 'ring', 'rings', 'rip', 'ripped', 'rise', 'rising', 'risk', 'rita', 'ritter', 'rival', 'river', 'riveting', 'road', 'rob', 'robbery', 'robbins', 'robert', 'roberts', 'robin', 'robinson', 'robot', 'robots', 'rochester', 'rock', 'rocket', 'rocks', 'rocky', 'roger', 'rogers', 'role', 'roles', 'roll', 'rolled', 'rolling', 'roman', 'romance', 'romantic', 'romero', 'romp', 'ron', 'room', 'rooms', 'rooney', 'root', 'roots', 'rose', 'ross', 'roth', 'rotten', 'rough', 'round', 'routine', 'row', 'roy', 'royal', 'rubber', 'rubbish', 'ruby', 'ruin', 'ruined', 'ruins', 'rukh', 'rule', 'rules', 'run', 'running', 'runs', 'rural', 'rush', 'rushed', 'russell', 'russian', 'ruth', 'ruthless', 'ryan', 'sabrina', 'sacrifice', 'sad', 'sadistic', 'sadly', 'sadness', 'safe', 'safety', 'saga', 'said', 'sake', 'sally', 'sam', 'samurai', 'san', 'sandler', 'sandra', 'santa', 'sappy', 'sarah', 'sat', 'satan', 'satire', 'satisfied', 'satisfy', 'satisfying', 'saturday', 'savage', 'save', 'saved', 'saves', 'saving', 'saw', 'say', 'saying', 'says', 'scale', 'scare', 'scarecrow', 'scared', 'scares', 'scary', 'scenario', 'scene', 'scenery', 'scenes', 'scheme', 'school', 'sci', 'science', 'scientific', 'scientist', 'scientists', 'scooby', 'scope', 'score', 'scores', 'scotland', 'scott', 'scottish', 'scream', 'screaming', 'screams', 'screen', 'screening', 'screenplay', 'screenwriter', 'script', 'scripted', 'scripts', 'scrooge', 'sea', 'seagal', 'sean', 'search', 'searching', 'season', 'seasons', 'seat', 'second', 'secondly', 'seconds', 'secret', 'secretary', 'secretly', 'secrets', 'section', 'security', 'see', 'seed', 'seeing', 'seek', 'seeking', 'seeks', 'seem', 'seemed', 'seemingly', 'seems', 'seen', 'sees', 'segment', 'segments', 'seldom', 'self', 'selfish', 'sell', 'sellers', 'selling', 'semi', 'send', 'sends', 'sense', 'senseless', 'sensitive', 'sent', 'sentence', 'sentimental', 'separate', 'september', 'sequel', 'sequels', 'sequence', 'sequences', 'serial', 'series', 'serious', 'seriously', 'serve', 'served', 'serves', 'service', 'serving', 'set', 'sets', 'setting', 'settings', 'settle', 'seven', 'seventies', 'several', 'severe', 'sex', 'sexual', 'sexuality', 'sexually', 'sexy', 'sh', 'shadow', 'shadows', 'shake', 'shakespeare', 'shaky', 'shall', 'shallow', 'shame', 'shanghai', 'shape', 'share', 'shark', 'sharp', 'shaw', 'shed', 'sheer', 'shelf', 'shelley', 'sheriff', 'shine', 'shines', 'shining', 'ship', 'shirley', 'shirt', 'shock', 'shocked', 'shocking', 'shoes', 'shoot', 'shooting', 'shoots', 'shop', 'short', 'shortly', 'shorts', 'shot', 'shots', 'show', 'showcase', 'showdown', 'showed', 'shower', 'showing', 'shown', 'shows', 'shut', 'shy', 'sick', 'sid', 'side', 'sidekick', 'sides', 'sidney', 'sight', 'sign', 'signed', 'significance', 'significant', 'signs', 'silence', 'silent', 'silly', 'silver', 'similar', 'similarities', 'similarly', 'simmons', 'simon', 'simple', 'simplicity', 'simplistic', 'simply', 'simpson', 'sin', 'sinatra', 'since', 'sincere', 'sing', 'singer', 'singing', 'single', 'sings', 'sinister', 'sink', 'sir', 'sirk', 'sissy', 'sister', 'sisters', 'sit', 'sitcom', 'site', 'sits', 'sitting', 'situation', 'situations', 'six', 'sixties', 'size', 'skill', 'skills', 'skin', 'skip', 'sky', 'slap', 'slapstick', 'slasher', 'slaughter', 'slave', 'sleazy', 'sleep', 'sleeping', 'slice', 'slick', 'slight', 'slightest', 'slightly', 'sloppy', 'slow', 'slowly', 'small', 'smaller', 'smart', 'smile', 'smiling', 'smith', 'smoke', 'smoking', 'smooth', 'snake', 'sneak', 'snl', 'snow', 'soap', 'soccer', 'social', 'society', 'soderbergh', 'soft', 'sold', 'soldier', 'soldiers', 'sole', 'solely', 'solid', 'solo', 'solution', 'solve', 'somebody', 'somehow', 'someone', 'something', 'sometimes', 'somewhat', 'somewhere', 'son', 'song', 'songs', 'sons', 'soon', 'sophisticated', 'sorry', 'sort', 'sorts', 'soul', 'souls', 'sound', 'sounded', 'sounding', 'sounds', 'soundtrack', 'source', 'south', 'southern', 'soviet', 'space', 'spacey', 'spain', 'spanish', 'spare', 'speak', 'speaking', 'speaks', 'special', 'specially', 'species', 'specific', 'specifically', 'spectacular', 'speech', 'speed', 'spell', 'spend', 'spending', 'spends', 'spent', 'spider', 'spielberg', 'spike', 'spin', 'spirit', 'spirited', 'spirits', 'spiritual', 'spite', 'splatter', 'splendid', 'split', 'spock', 'spoil', 'spoiled', 'spoiler', 'spoilers', 'spoke', 'spoken', 'spoof', 'spooky', 'sport', 'sports', 'spot', 'spots', 'spread', 'spring', 'spy', 'square', 'st', 'staff', 'stage', 'staged', 'stale', 'stallone', 'stan', 'stand', 'standard', 'standards', 'standing', 'stands', 'stanley', 'stanwyck', 'star', 'stargate', 'staring', 'starred', 'starring', 'stars', 'start', 'started', 'starting', 'starts', 'state', 'stated', 'statement', 'states', 'station', 'status', 'stay', 'stayed', 'staying', 'stays', 'steal', 'stealing', 'steals', 'steel', 'stellar', 'step', 'stephen', 'steps', 'stereotype', 'stereotypes', 'stereotypical', 'steve', 'steven', 'stevens', 'stewart', 'stick', 'sticks', 'stiff', 'still', 'stiller', 'stilted', 'stinker', 'stinks', 'stock', 'stole', 'stolen', 'stomach', 'stone', 'stood', 'stooges', 'stop', 'stopped', 'stops', 'store', 'stories', 'storm', 'story', 'storyline', 'storytelling', 'straight', 'strange', 'strangely', 'stranger', 'strangers', 'streep', 'street', 'streets', 'streisand', 'strength', 'stress', 'stretch', 'stretched', 'strictly', 'strike', 'strikes', 'striking', 'string', 'strip', 'strong', 'stronger', 'strongly', 'struck', 'structure', 'struggle', 'struggles', 'struggling', 'stuart', 'stuck', 'student', 'students', 'studio', 'studios', 'study', 'stuff', 'stunned', 'stunning', 'stunt', 'stunts', 'stupid', 'stupidity', 'style', 'styles', 'stylish', 'sub', 'subject', 'subjects', 'subplot', 'subplots', 'subsequent', 'substance', 'subtitles', 'subtle', 'subtlety', 'succeed', 'succeeded', 'succeeds', 'success', 'successful', 'successfully', 'suck', 'sucked', 'sucks', 'sudden', 'suddenly', 'sue', 'suffer', 'suffered', 'suffering', 'suffers', 'suffice', 'suggest', 'suggested', 'suggests', 'suicide', 'suit', 'suitable', 'suited', 'suits', 'sullivan', 'sum', 'summary', 'summer', 'sun', 'sunday', 'sunshine', 'super', 'superb', 'superbly', 'superficial', 'superhero', 'superior', 'superman', 'supernatural', 'support', 'supported', 'supporting', 'suppose', 'supposed', 'supposedly', 'sure', 'surely', 'surface', 'surfing', 'surprise', 'surprised', 'surprises', 'surprising', 'surprisingly', 'surreal', 'surrounded', 'surrounding', 'survival', 'survive', 'survived', 'surviving', 'survivor', 'survivors', 'susan', 'suspect', 'suspects', 'suspend', 'suspense', 'suspenseful', 'suspicious', 'sutherland', 'swear', 'swedish', 'sweet', 'swim', 'swimming', 'switch', 'sword', 'symbolism', 'sympathetic', 'sympathy', 'synopsis', 'system', 'table', 'tad', 'tag', 'take', 'taken', 'takes', 'taking', 'tale', 'talent', 'talented', 'talents', 'tales', 'talk', 'talked', 'talking', 'talks', 'tall', 'tame', 'tank', 'tap', 'tape', 'tarantino', 'target', 'tarzan', 'task', 'taste', 'taught', 'taxi', 'taylor', 'tea', 'teach', 'teacher', 'teaching', 'team', 'tear', 'tears', 'tech', 'technical', 'technically', 'technicolor', 'technique', 'techniques', 'technology', 'ted', 'tedious', 'teen', 'teenage', 'teenager', 'teenagers', 'teens', 'teeth', 'television', 'tell', 'telling', 'tells', 'temple', 'ten', 'tend', 'tender', 'tends', 'tense', 'tension', 'term', 'terms', 'terrible', 'terribly', 'terrific', 'terrifying', 'territory', 'terror', 'terrorist', 'terrorists', 'terry', 'test', 'testament', 'texas', 'text', 'thank', 'thankfully', 'thanks', 'thats', 'theater', 'theaters', 'theatre', 'theatrical', 'theme', 'themes', 'theory', 'therefore', 'thick', 'thief', 'thin', 'thing', 'things', 'think', 'thinking', 'thinks', 'third', 'thirty', 'thomas', 'thompson', 'thoroughly', 'though', 'thought', 'thoughtful', 'thoughts', 'thousand', 'thousands', 'threat', 'threatening', 'three', 'threw', 'thrill', 'thriller', 'thrillers', 'thrilling', 'thrills', 'throat', 'throughout', 'throw', 'throwing', 'thrown', 'throws', 'thru', 'thugs', 'thumbs', 'thus', 'ticket', 'tie', 'tied', 'ties', 'tiger', 'tight', 'till', 'tim', 'time', 'timeless', 'times', 'timing', 'timon', 'timothy', 'tiny', 'tired', 'tiresome', 'titanic', 'title', 'titled', 'titles', 'today', 'todd', 'together', 'toilet', 'told', 'tom', 'tomatoes', 'tommy', 'tone', 'tongue', 'tonight', 'tons', 'tony', 'took', 'tooth', 'top', 'topic', 'topless', 'torn', 'torture', 'tortured', 'total', 'totally', 'touch', 'touched', 'touches', 'touching', 'tough', 'tour', 'toward', 'towards', 'town', 'toy', 'toys', 'track', 'tracks', 'tracy', 'trade', 'trademark', 'tradition', 'traditional', 'tragedy', 'tragic', 'trail', 'trailer', 'trailers', 'train', 'trained', 'training', 'transfer', 'transformation', 'transition', 'translation', 'trap', 'trapped', 'trash', 'trashy', 'travel', 'traveling', 'travels', 'travesty', 'treasure', 'treat', 'treated', 'treatment', 'treats', 'tree', 'trees', 'trek', 'tremendous', 'trial', 'tribe', 'tribute', 'trick', 'tricks', 'tried', 'tries', 'trilogy', 'trio', 'trip', 'trite', 'triumph', 'troops', 'trouble', 'troubled', 'troubles', 'truck', 'true', 'truly', 'trust', 'truth', 'try', 'trying', 'tune', 'tunes', 'turkey', 'turn', 'turned', 'turner', 'turning', 'turns', 'tv', 'twelve', 'twenty', 'twice', 'twilight', 'twin', 'twins', 'twist', 'twisted', 'twists', 'two', 'type', 'types', 'typical', 'typically', 'ugly', 'uk', 'ultimate', 'ultimately', 'ultra', 'un', 'unable', 'unaware', 'unbearable', 'unbelievable', 'unbelievably', 'uncle', 'uncomfortable', 'unconvincing', 'underground', 'underlying', 'underrated', 'understand', 'understandable', 'understanding', 'understated', 'understood', 'undoubtedly', 'uneven', 'unexpected', 'unfair', 'unfolds', 'unforgettable', 'unfortunate', 'unfortunately', 'unfunny', 'unhappy', 'uninspired', 'unintentional', 'unintentionally', 'uninteresting', 'union', 'unique', 'unit', 'united', 'universal', 'universe', 'university', 'unknown', 'unless', 'unlike', 'unlikely', 'unnecessary', 'unoriginal', 'unpleasant', 'unpredictable', 'unreal', 'unrealistic', 'unseen', 'unsettling', 'unusual', 'unwatchable', 'uplifting', 'upon', 'upper', 'ups', 'upset', 'urban', 'urge', 'us', 'usa', 'use', 'used', 'useful', 'useless', 'user', 'uses', 'using', 'ustinov', 'usual', 'usually', 'utter', 'utterly', 'uwe', 'vacation', 'vague', 'vaguely', 'valentine', 'valley', 'valuable', 'value', 'values', 'vampire', 'vampires', 'van', 'variety', 'various', 'vast', 'vegas', 'vehicle', 'vengeance', 'verhoeven', 'version', 'versions', 'versus', 'veteran', 'vhs', 'via', 'vice', 'vicious', 'victim', 'victims', 'victor', 'victoria', 'victory', 'video', 'videos', 'vietnam', 'view', 'viewed', 'viewer', 'viewers', 'viewing', 'viewings', 'views', 'village', 'villain', 'villains', 'vincent', 'violence', 'violent', 'virgin', 'virginia', 'virtually', 'virus', 'visible', 'vision', 'visit', 'visits', 'visual', 'visually', 'visuals', 'vivid', 'voice', 'voiced', 'voices', 'voight', 'von', 'vote', 'vs', 'vulnerable', 'wacky', 'wait', 'waited', 'waiting', 'waitress', 'wake', 'walk', 'walked', 'walken', 'walker', 'walking', 'walks', 'wall', 'wallace', 'walls', 'walsh', 'walter', 'wandering', 'wang', 'wanna', 'wannabe', 'want', 'wanted', 'wanting', 'wants', 'war', 'ward', 'warm', 'warming', 'warmth', 'warn', 'warned', 'warner', 'warning', 'warren', 'warrior', 'warriors', 'wars', 'washington', 'waste', 'wasted', 'wasting', 'watch', 'watchable', 'watched', 'watches', 'watching', 'water', 'waters', 'watson', 'wave', 'waves', 'way', 'wayne', 'ways', 'weak', 'weakest', 'wealth', 'wealthy', 'weapon', 'weapons', 'wear', 'wearing', 'wears', 'web', 'website', 'wedding', 'week', 'weekend', 'weeks', 'weight', 'weird', 'welcome', 'well', 'welles', 'wells', 'wendy', 'went', 'werewolf', 'wes', 'west', 'western', 'westerns', 'wet', 'whale', 'whatever', 'whats', 'whatsoever', 'whenever', 'whereas', 'whether', 'whilst', 'white', 'whoever', 'whole', 'wholly', 'whoopi', 'whose', 'wicked', 'wide', 'widely', 'widmark', 'widow', 'wife', 'wild', 'wilderness', 'william', 'williams', 'willie', 'willing', 'willis', 'wilson', 'win', 'wind', 'window', 'winds', 'wing', 'winner', 'winning', 'wins', 'winter', 'winters', 'wisdom', 'wise', 'wish', 'wished', 'wishes', 'wishing', 'wit', 'witch', 'witches', 'within', 'without', 'witness', 'witnessed', 'witnesses', 'witty', 'wives', 'wizard', 'wolf', 'woman', 'women', 'wonder', 'wondered', 'wonderful', 'wonderfully', 'wondering', 'wonders', 'wont', 'woo', 'wood', 'wooden', 'woods', 'woody', 'word', 'words', 'wore', 'work', 'worked', 'worker', 'workers', 'working', 'works', 'world', 'worlds', 'worn', 'worried', 'worry', 'worse', 'worst', 'worth', 'worthless', 'worthwhile', 'worthy', 'would', 'wound', 'wounded', 'wow', 'wrap', 'wrapped', 'wreck', 'wrestling', 'write', 'writer', 'writers', 'writes', 'writing', 'written', 'wrong', 'wrote', 'wwii', 'ya', 'yard', 'yeah', 'year', 'years', 'yelling', 'yellow', 'yes', 'yesterday', 'yet', 'york', 'young', 'younger', 'youth', 'zero', 'zizek', 'zombie', 'zombies', 'zone']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "contained-blair",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 5000)"
      ]
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "peaceful-learning",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<25000x5000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1978769 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "technological-struggle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<25000x5000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1963651 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 394,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_vec = vectorizer.fit_transform(x_test)\n",
    "x_test_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "fatal-citation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 5000)"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_vec.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "responsible-cheat",
   "metadata": {},
   "source": [
    "## 【問題3】TF-IDFを用いた学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "formal-heating",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(random_state=0)"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "clf.fit(x_train_vec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "sufficient-shannon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = clf.predict(x_test_vec)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "creative-helicopter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.53748"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smaller-signature",
   "metadata": {},
   "source": [
    "## 【問題4】TF-IDFのスクラッチ実装"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "talented-transcript",
   "metadata": {},
   "source": [
    "### 標準式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "id": "exciting-timothy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf_usual(X):\n",
    "\n",
    "    \"\"\"\n",
    "    listを受け取りuni-gramのTF-IDFを返す\n",
    "    \"\"\"\n",
    "    \n",
    "    word_list = []\n",
    "\n",
    "    for i in range(len(X)):\n",
    "\n",
    "        split = X[i].replace(\"!\", \"\").split(\" \")\n",
    "        word_list.extend(split)\n",
    "        \n",
    "    word_list = list(set(word_list))\n",
    "    word_list.sort()\n",
    "    sentence_counts = len(X)\n",
    "    out = np.zeros((sentence_counts, len(word_list)))\n",
    "\n",
    "\n",
    "    for j in range(sentence_counts):\n",
    "\n",
    "        for sentence in X[j].replace(\"!\", \"\").split(\" \"):\n",
    "\n",
    "            for i, word in enumerate(word_list):\n",
    "                \n",
    "                if sentence==word:\n",
    "                    out[j, i] += 1\n",
    "    \n",
    "    \n",
    "    out = out.astype(np.float64)\n",
    "    \n",
    "    # TF\n",
    "    for j in range(sentence_counts):\n",
    "        for i in range(len(word_list)): \n",
    "            if  out[j, i] >= 1e-7:\n",
    "                out[j, i] /= len(X[j].split(\" \"))\n",
    "            \n",
    "    #print(out)\n",
    "    \n",
    "    # IDF        \n",
    "    elem_nonzero = np.count_nonzero(out, axis=0)\n",
    "    \n",
    "    for i in range(len(word_list)):\n",
    "        for j in range(sentence_counts):\n",
    "            if out[j, i] >= 1e-7:\n",
    "                out[j, i] *= np.log(sentence_counts / elem_nonzero[i])\n",
    "                \n",
    "    #print(out)\n",
    "                \n",
    "    df =  pd.DataFrame(out, columns=word_list)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "id": "competitive-standing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>I</th>\n",
       "      <th>SOOOO</th>\n",
       "      <th>This</th>\n",
       "      <th>What</th>\n",
       "      <th>a</th>\n",
       "      <th>best</th>\n",
       "      <th>ever</th>\n",
       "      <th>funny</th>\n",
       "      <th>is</th>\n",
       "      <th>movie</th>\n",
       "      <th>never</th>\n",
       "      <th>this</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.219722</td>\n",
       "      <td>0.219722</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.219722</td>\n",
       "      <td>0.219722</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.219722</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.219722</td>\n",
       "      <td>0.219722</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.219722</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.219722</td>\n",
       "      <td>0.219722</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.219722</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          I     SOOOO      This      What         a      best      ever  \\\n",
       "0  0.000000  0.219722  0.219722  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.219722  0.000000  0.000000  0.219722  0.219722  0.000000  0.000000   \n",
       "2  0.000000  0.000000  0.000000  0.000000  0.000000  0.219722  0.219722   \n",
       "\n",
       "      funny        is  movie     never      this  \n",
       "0  0.219722  0.219722    0.0  0.000000  0.000000  \n",
       "1  0.000000  0.000000    0.0  0.219722  0.000000  \n",
       "2  0.000000  0.000000    0.0  0.000000  0.219722  "
      ]
     },
     "execution_count": 541,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = tf_idf_usual(mini_dataset2)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blessed-hospital",
   "metadata": {},
   "source": [
    "### scilit-learn式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "id": "transparent-gallery",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf_sklearn(X):\n",
    "\n",
    "    \"\"\"\n",
    "    listを受け取りuni-gramのTF-IDFを返す\n",
    "    \"\"\"\n",
    "    \n",
    "    word_list = []\n",
    "\n",
    "    for i in range(len(X)):\n",
    "\n",
    "        split = X[i].replace(\"!\", \"\").split(\" \")\n",
    "        word_list.extend(split)\n",
    "        \n",
    "    word_list = list(set(word_list))\n",
    "    word_list.sort()\n",
    "    sentence_counts = len(X)\n",
    "    out = np.zeros((sentence_counts, len(word_list)))\n",
    "\n",
    "\n",
    "    for j in range(sentence_counts):\n",
    "\n",
    "        for sentence in X[j].replace(\"!\", \"\").split(\" \"):\n",
    "\n",
    "            for i, word in enumerate(word_list):\n",
    "                \n",
    "                if sentence==word:\n",
    "                    out[j, i] += 1\n",
    "    \n",
    "    \n",
    "    out = out.astype(np.float64)\n",
    "    \n",
    "    # TF\n",
    "    #for j in range(sentence_counts):\n",
    "    #    for i in range(len(word_list)): \n",
    "    #        if  out[j, i] >= 1e-7:\n",
    "    #            out[j, i] /= len(X[j].split(\" \"))\n",
    "            \n",
    "    #print(out)\n",
    "    \n",
    "    # IDF        \n",
    "    elem_nonzero = np.count_nonzero(out, axis=0)\n",
    "    \n",
    "    for i in range(len(word_list)):\n",
    "        for j in range(sentence_counts):\n",
    "            if out[j, i] >= 1e-7:\n",
    "                out[j, i] *= np.log(1 + sentence_counts / 1 + elem_nonzero[i]) + 1\n",
    "                \n",
    "    #print(out)\n",
    "                \n",
    "    df =  pd.DataFrame(out, columns=word_list)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "id": "controlling-neighbor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>I</th>\n",
       "      <th>SOOOO</th>\n",
       "      <th>This</th>\n",
       "      <th>What</th>\n",
       "      <th>a</th>\n",
       "      <th>best</th>\n",
       "      <th>ever</th>\n",
       "      <th>funny</th>\n",
       "      <th>is</th>\n",
       "      <th>movie</th>\n",
       "      <th>never</th>\n",
       "      <th>this</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.609438</td>\n",
       "      <td>2.609438</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.609438</td>\n",
       "      <td>2.609438</td>\n",
       "      <td>2.94591</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.609438</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.609438</td>\n",
       "      <td>2.609438</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.94591</td>\n",
       "      <td>2.609438</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.609438</td>\n",
       "      <td>2.609438</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.89182</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.609438</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          I     SOOOO      This      What         a      best      ever  \\\n",
       "0  0.000000  2.609438  2.609438  0.000000  0.000000  0.000000  0.000000   \n",
       "1  2.609438  0.000000  0.000000  2.609438  2.609438  0.000000  0.000000   \n",
       "2  0.000000  0.000000  0.000000  0.000000  0.000000  2.609438  2.609438   \n",
       "\n",
       "      funny        is    movie     never      this  \n",
       "0  2.609438  2.609438  2.94591  0.000000  0.000000  \n",
       "1  0.000000  0.000000  2.94591  2.609438  0.000000  \n",
       "2  0.000000  0.000000  5.89182  0.000000  2.609438  "
      ]
     },
     "execution_count": 543,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_sklearn(mini_dataset2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broken-dining",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "id": "otherwise-single",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.0.1-cp37-cp37m-macosx_10_9_x86_64.whl (23.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 23.9 MB 24.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting smart-open>=1.8.1\n",
      "  Downloading smart_open-5.0.0-py3-none-any.whl (56 kB)\n",
      "\u001b[K     |████████████████████████████████| 56 kB 7.2 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.11.3 in /Users/teruitakahiro/opt/anaconda3/envs/data_science/lib/python3.7/site-packages (from gensim) (1.19.3)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /Users/teruitakahiro/opt/anaconda3/envs/data_science/lib/python3.7/site-packages (from gensim) (1.6.2)\n",
      "Installing collected packages: smart-open, gensim\n",
      "Successfully installed gensim-4.0.1 smart-open-5.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "id": "temporal-render",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "語彙の一覧 : dict_keys(['very', 'bad', 'good', 'is', 'this', 'a', 'film', 'movie'])\n",
      "veryのベクトル : \n",
      "[-5.3622725e-04  2.3643016e-04  5.1033497e-03  9.0092728e-03\n",
      " -9.3029495e-03 -7.1168090e-03  6.4588715e-03  8.9729885e-03\n",
      " -5.0154282e-03 -3.7633730e-03  7.3805046e-03 -1.5334726e-03\n",
      " -4.5366143e-03  6.5540504e-03 -4.8601604e-03 -1.8160177e-03\n",
      "  2.8765798e-03  9.9187379e-04 -8.2852151e-03 -9.4488189e-03\n",
      "  7.3117660e-03  5.0702621e-03  6.7576934e-03  7.6286553e-04\n",
      "  6.3508893e-03 -3.4053659e-03 -9.4640255e-04  5.7685734e-03\n",
      " -7.5216386e-03 -3.9361049e-03 -7.5115822e-03 -9.3004224e-04\n",
      "  9.5381187e-03 -7.3191668e-03 -2.3337698e-03 -1.9377422e-03\n",
      "  8.0774352e-03 -5.9308959e-03  4.5161247e-05 -4.7537349e-03\n",
      " -9.6035507e-03  5.0072931e-03 -8.7595871e-03 -4.3918253e-03\n",
      " -3.5099984e-05 -2.9618264e-04 -7.6612402e-03  9.6147414e-03\n",
      "  4.9820566e-03  9.2331432e-03 -8.1579182e-03  4.4957972e-03\n",
      " -4.1370774e-03  8.2453492e-04  8.4986184e-03 -4.4621779e-03\n",
      "  4.5175003e-03 -6.7869616e-03 -3.5484887e-03  9.3985079e-03\n",
      " -1.5776539e-03  3.2137157e-04 -4.1406299e-03 -7.6826881e-03\n",
      " -1.5080094e-03  2.4697948e-03 -8.8802812e-04  5.5336617e-03\n",
      " -2.7429771e-03  2.2600652e-03  5.4557943e-03  8.3459523e-03\n",
      " -1.4537406e-03 -9.2081428e-03  4.3705511e-03  5.7178497e-04\n",
      "  7.4419067e-03 -8.1328390e-04 -2.6384138e-03 -8.7530091e-03\n",
      " -8.5655687e-04  2.8265619e-03  5.4014279e-03  7.0526553e-03\n",
      " -5.7031228e-03  1.8588186e-03  6.0888622e-03 -4.7980524e-03\n",
      " -3.1072616e-03  6.7976285e-03  1.6314745e-03  1.8991709e-04\n",
      "  3.4736372e-03  2.1777629e-04  9.6188262e-03  5.0606038e-03\n",
      " -8.9173913e-03 -7.0415614e-03  9.0145587e-04  6.3925339e-03]\n",
      "badのベクトル : \n",
      "[-8.6196875e-03  3.6657380e-03  5.1898835e-03  5.7419371e-03\n",
      "  7.4669169e-03 -6.1676763e-03  1.1056137e-03  6.0472824e-03\n",
      " -2.8400517e-03 -6.1735227e-03 -4.1022300e-04 -8.3689503e-03\n",
      " -5.6000138e-03  7.1045374e-03  3.3525396e-03  7.2256685e-03\n",
      "  6.8002464e-03  7.5307419e-03 -3.7891555e-03 -5.6180713e-04\n",
      "  2.3483753e-03 -4.5190332e-03  8.3887316e-03 -9.8581649e-03\n",
      "  6.7646410e-03  2.9144168e-03 -4.9328329e-03  4.3981862e-03\n",
      " -1.7395759e-03  6.7113829e-03  9.9648498e-03 -4.3624449e-03\n",
      " -5.9933902e-04 -5.6956387e-03  3.8508223e-03  2.7866268e-03\n",
      "  6.8910765e-03  6.1010956e-03  9.5384959e-03  9.2734173e-03\n",
      "  7.8980681e-03 -6.9895051e-03 -9.1558648e-03 -3.5575390e-04\n",
      " -3.0998420e-03  7.8943158e-03  5.9385728e-03 -1.5456629e-03\n",
      "  1.5109634e-03  1.7900396e-03  7.8175711e-03 -9.5101884e-03\n",
      " -2.0553112e-04  3.4691954e-03 -9.3897345e-04  8.3817719e-03\n",
      "  9.0107825e-03  6.5365052e-03 -7.1162224e-04  7.7104042e-03\n",
      " -8.5343365e-03  3.2071066e-03 -4.6379971e-03 -5.0889566e-03\n",
      "  3.5896183e-03  5.3703380e-03  7.7695129e-03 -5.7665063e-03\n",
      "  7.4333595e-03  6.6254949e-03 -3.7098003e-03 -8.7456414e-03\n",
      "  5.4374672e-03  6.5097548e-03 -7.8755140e-04 -6.7098569e-03\n",
      " -7.0859264e-03 -2.4970602e-03  5.1432536e-03 -3.6652375e-03\n",
      " -9.3700597e-03  3.8267397e-03  4.8844791e-03 -6.4285635e-03\n",
      "  1.2085581e-03 -2.0748782e-03  2.4402141e-05 -9.8835090e-03\n",
      "  2.6920033e-03 -4.7501065e-03  1.0876465e-03 -1.5762257e-03\n",
      "  2.1966719e-03 -7.8815771e-03 -2.7171851e-03  2.6631975e-03\n",
      "  5.3466819e-03 -2.3915148e-03 -9.5100952e-03  4.5058774e-03]\n",
      "goodのベクトル : \n",
      "[ 9.4563962e-05  3.0773187e-03 -6.8126465e-03 -1.3754654e-03\n",
      "  7.6685809e-03  7.3464084e-03 -3.6732983e-03  2.6427007e-03\n",
      " -8.3171297e-03  6.2054847e-03 -4.6373224e-03 -3.1641079e-03\n",
      "  9.3113566e-03  8.7338447e-04  7.4907015e-03 -6.0740639e-03\n",
      "  5.1605059e-03  9.9228211e-03 -8.4573915e-03 -5.1356913e-03\n",
      " -7.0648384e-03 -4.8626517e-03 -3.7785650e-03 -8.5362010e-03\n",
      "  7.9556061e-03 -4.8439382e-03  8.4236125e-03  5.2625705e-03\n",
      " -6.5500261e-03  3.9578700e-03  5.4701497e-03 -7.4265362e-03\n",
      " -7.4057197e-03 -2.4752307e-03 -8.6257271e-03 -1.5815735e-03\n",
      " -4.0343284e-04  3.2996845e-03  1.4418793e-03 -8.8142155e-04\n",
      " -5.5940580e-03  1.7303658e-03 -8.9737179e-04  6.7936899e-03\n",
      "  3.9735888e-03  4.5294715e-03  1.4343048e-03 -2.6998566e-03\n",
      " -4.3668128e-03 -1.0320758e-03  1.4370275e-03 -2.6460099e-03\n",
      " -7.0737838e-03 -7.8053069e-03 -9.1217877e-03 -5.9351707e-03\n",
      " -1.8474245e-03 -4.3238713e-03 -6.4606713e-03 -3.7173224e-03\n",
      "  4.2891572e-03 -3.7390448e-03  8.3781742e-03  1.5339922e-03\n",
      " -7.2423196e-03  9.4337985e-03  7.6312111e-03  5.4932809e-03\n",
      " -6.8488456e-03  5.8226776e-03  4.0090918e-03  5.1853680e-03\n",
      "  4.2559002e-03  1.9397545e-03 -3.1701636e-03  8.3538434e-03\n",
      "  9.6121784e-03  3.7926030e-03 -2.8369951e-03  7.1263312e-06\n",
      "  1.2188172e-03 -8.4583256e-03 -8.2239462e-03 -2.3101569e-04\n",
      "  1.2372875e-03 -5.7433820e-03 -4.7252751e-03 -7.3460746e-03\n",
      "  8.3286138e-03  1.2129784e-04 -4.5093987e-03  5.7017040e-03\n",
      "  9.1800140e-03 -4.0998720e-03  7.9646800e-03  5.3754328e-03\n",
      "  5.8791232e-03  5.1259040e-04  8.2130842e-03 -7.0190406e-03]\n",
      "isのベクトル : \n",
      "[-8.2427301e-03  9.2994133e-03 -1.9766216e-04 -1.9672900e-03\n",
      "  4.6036579e-03 -4.0953415e-03  2.7431303e-03  6.9400100e-03\n",
      "  6.0654641e-03 -7.5108428e-03  9.3824090e-03  4.6718367e-03\n",
      "  3.9661438e-03 -6.2435446e-03  8.4600309e-03 -2.1501796e-03\n",
      "  8.8252425e-03 -5.3620362e-03 -8.1294719e-03  6.8246005e-03\n",
      "  1.6712032e-03 -2.1985238e-03  9.5136594e-03  9.4939135e-03\n",
      " -9.7741093e-03  2.5052442e-03  6.1567309e-03  3.8724807e-03\n",
      "  2.0227986e-03  4.3050322e-04  6.7363441e-04 -3.8206603e-03\n",
      " -7.1402951e-03 -2.0888865e-03  3.9239223e-03  8.8187382e-03\n",
      "  9.2592081e-03 -5.9759752e-03 -9.4027296e-03  9.7644376e-03\n",
      "  3.4298061e-03  5.1661478e-03  6.2823831e-03 -2.8042800e-03\n",
      "  7.3227482e-03  2.8302893e-03  2.8710212e-03 -2.3803860e-03\n",
      " -3.1282692e-03 -2.3701577e-03  4.2764619e-03  7.6058386e-05\n",
      " -9.5843384e-03 -9.6656047e-03 -6.1482340e-03 -1.2857041e-04\n",
      "  1.9974285e-03  9.4320262e-03  5.5843848e-03 -4.2907246e-03\n",
      "  2.7831728e-04  4.9643898e-03  7.6983576e-03 -1.1442305e-03\n",
      "  4.3234476e-03 -5.8144168e-03 -8.0419565e-04  8.1000999e-03\n",
      " -2.3600797e-03 -9.6635157e-03  5.7792957e-03 -3.9298469e-03\n",
      " -1.2228804e-03  9.9805789e-03 -2.2563648e-03 -4.7570956e-03\n",
      " -5.3294208e-03  6.9809328e-03 -5.7089091e-03  2.1136750e-03\n",
      " -5.2556940e-03  6.1207511e-03  4.3573342e-03  2.6063700e-03\n",
      " -1.4910934e-03 -2.7460819e-03  8.9929923e-03  5.2158060e-03\n",
      " -2.1625343e-03 -9.4703697e-03 -7.4260985e-03 -1.0637493e-03\n",
      " -7.9495210e-04 -2.5629252e-03  9.6827801e-03 -4.5852474e-04\n",
      "  5.8737979e-03 -7.4476348e-03 -2.5060906e-03 -5.5498998e-03]\n",
      "thisのベクトル : \n",
      "[-0.00713902  0.00124103 -0.00717672 -0.00224462  0.0037193   0.00583312\n",
      "  0.00119818  0.00210273 -0.00411039  0.00722533 -0.00630704  0.00464721\n",
      " -0.00821997  0.00203647 -0.00497705 -0.00424769 -0.00310899  0.00565521\n",
      "  0.0057984  -0.00497465  0.00077333 -0.00849578  0.00780981  0.00925729\n",
      " -0.00274233  0.00080022  0.00074665  0.00547788 -0.00860608  0.00058445\n",
      "  0.00686942  0.00223159  0.00112468 -0.00932216  0.00848237 -0.00626413\n",
      " -0.00299237  0.00349379 -0.00077263  0.00141129  0.00178199 -0.0068289\n",
      " -0.00972481  0.00904058  0.00619805 -0.00691293  0.00340348  0.00020606\n",
      "  0.00475374 -0.00711994  0.00402695  0.00434743  0.00995737 -0.00447374\n",
      " -0.00138927 -0.00731732 -0.00969783 -0.00908026 -0.00102276 -0.00650329\n",
      "  0.00484973 -0.00616403  0.00251919  0.00073944 -0.00339216 -0.00097922\n",
      "  0.00997912  0.00914589 -0.00446183  0.00908303 -0.00564176  0.00593092\n",
      " -0.00309722  0.00343175  0.00301723  0.00690046 -0.00237388  0.00877504\n",
      "  0.00758943 -0.00954765 -0.00800821 -0.0076379   0.00292326 -0.00279472\n",
      " -0.00692952 -0.00812826  0.00830918  0.00199049 -0.00932802 -0.00479272\n",
      "  0.00313674 -0.00471321  0.00528084 -0.00423344  0.00264179 -0.00804569\n",
      "  0.00620989  0.00481889  0.00078719  0.00301345]\n",
      "aのベクトル : \n",
      "[-8.7274835e-03  2.1301603e-03 -8.7354420e-04 -9.3190884e-03\n",
      " -9.4281435e-03 -1.4107180e-03  4.4324086e-03  3.7040710e-03\n",
      " -6.4986944e-03 -6.8730689e-03 -4.9994136e-03 -2.2868442e-03\n",
      " -7.2502876e-03 -9.6033188e-03 -2.7436304e-03 -8.3628418e-03\n",
      " -6.0388758e-03 -5.6709289e-03 -2.3441387e-03 -1.7069983e-03\n",
      " -8.9569995e-03 -7.3519943e-04  8.1525063e-03  7.6904297e-03\n",
      " -7.2061159e-03 -3.6668323e-03  3.1185509e-03 -9.5707225e-03\n",
      "  1.4764380e-03  6.5244650e-03  5.7464195e-03 -8.7630628e-03\n",
      " -4.5171450e-03 -8.1401607e-03  4.5955181e-05  9.2636319e-03\n",
      "  5.9733056e-03  5.0673080e-03  5.0610616e-03 -3.2429171e-03\n",
      "  9.5521836e-03 -7.3564244e-03 -7.2703888e-03 -2.2653891e-03\n",
      " -7.7856064e-04 -3.2161046e-03 -5.9258699e-04  7.4888230e-03\n",
      " -6.9751980e-04 -1.6249418e-03  2.7443981e-03 -8.3591007e-03\n",
      "  7.8558037e-03  8.5361032e-03 -9.5840879e-03  2.4462652e-03\n",
      "  9.9049713e-03 -7.6658037e-03 -6.9669201e-03 -7.7365185e-03\n",
      "  8.3959224e-03 -6.8133592e-04  9.1444086e-03 -8.1582209e-03\n",
      "  3.7430834e-03  2.6350426e-03  7.4271200e-04  2.3276759e-03\n",
      " -7.4690939e-03 -9.3583753e-03  2.3545765e-03  6.1484552e-03\n",
      "  7.9856869e-03  5.7358933e-03 -7.7733753e-04  8.3061643e-03\n",
      " -9.3363142e-03  3.4061312e-03  2.6675223e-04  3.8572431e-03\n",
      "  7.3857834e-03 -6.7251683e-03  5.5844807e-03 -9.5222257e-03\n",
      " -8.0446003e-04 -8.6887386e-03 -5.0986744e-03  9.2892265e-03\n",
      " -1.8582630e-03  2.9144264e-03  9.0712784e-03  8.9381309e-03\n",
      " -8.2084350e-03 -3.0123137e-03  9.8866057e-03  5.1044296e-03\n",
      " -1.5880871e-03 -8.6920215e-03  2.9615164e-03 -6.6758990e-03]\n",
      "filmのベクトル : \n",
      "[ 8.13237298e-03 -4.45739133e-03 -1.06837181e-03  1.00637740e-03\n",
      " -1.91116342e-04  1.14819047e-03  6.11393712e-03 -2.02729861e-05\n",
      " -3.24600725e-03 -1.51074864e-03  5.89737203e-03  1.51412003e-03\n",
      " -7.24272279e-04  9.33336187e-03 -4.92134504e-03 -8.38421343e-04\n",
      "  9.17552505e-03  6.74951170e-03  1.50287373e-03 -8.88267159e-03\n",
      "  1.14876044e-03 -2.28828425e-03  9.36835352e-03  1.20994169e-03\n",
      "  1.49008224e-03  2.40643881e-03 -1.83602958e-03 -4.99969767e-03\n",
      "  2.32432416e-04 -2.01420672e-03  6.60101557e-03  8.94023385e-03\n",
      " -6.74763985e-04  2.97705084e-03 -6.10773079e-03  1.69934495e-03\n",
      " -6.92632049e-03 -8.69413558e-03 -5.90027822e-03 -8.95658694e-03\n",
      "  7.27768475e-03 -5.77210495e-03  8.27645417e-03 -7.24363606e-03\n",
      "  3.42171662e-03  9.67511814e-03 -7.78554613e-03 -9.94518399e-03\n",
      " -4.32920130e-03 -2.68316525e-03 -2.71292753e-04 -8.83166213e-03\n",
      " -8.61766655e-03  2.80024461e-03 -8.20651092e-03 -9.06945113e-03\n",
      " -2.34049628e-03 -8.63191672e-03 -7.05673965e-03 -8.40125605e-03\n",
      " -3.01333872e-04 -4.56435699e-03  6.62725652e-03  1.52717833e-03\n",
      " -3.34151881e-03  6.10904722e-03 -6.01336174e-03 -4.65622777e-03\n",
      " -7.20760087e-03 -4.33663558e-03 -1.80935266e-03  6.48972252e-03\n",
      " -2.77042761e-03  4.91902791e-03  6.90452894e-03 -7.46379932e-03\n",
      "  4.56490740e-03  6.12705527e-03 -2.95451283e-03  6.62510330e-03\n",
      "  6.12595631e-03 -6.44356664e-03 -6.76463731e-03  2.53898953e-03\n",
      " -1.62383926e-03 -6.06520381e-03  9.49932821e-03 -5.13021229e-03\n",
      " -6.55418029e-03 -1.19887896e-04 -2.70146178e-03  4.44405858e-04\n",
      " -3.53750354e-03 -4.19335847e-04 -7.08624604e-04  8.22830945e-04\n",
      "  8.19491874e-03 -5.73678035e-03 -1.65954989e-03  5.57167595e-03]\n",
      "movieのベクトル : \n",
      "[ 8.1681199e-03 -4.4430327e-03  8.9854337e-03  8.2536647e-03\n",
      " -4.4352221e-03  3.0310394e-04  4.2744898e-03 -3.9263200e-03\n",
      " -5.5599664e-03 -6.5123225e-03 -6.7073823e-04 -2.9592274e-04\n",
      "  4.4630836e-03 -2.4740554e-03 -1.7261028e-04  2.4618744e-03\n",
      "  4.8675989e-03 -3.0808449e-05 -6.3394094e-03 -9.2608072e-03\n",
      "  2.6657581e-05  6.6618943e-03  1.4660215e-03 -8.9665223e-03\n",
      " -7.9386048e-03  6.5519023e-03 -3.7856817e-03  6.2549924e-03\n",
      " -6.6810320e-03  8.4796622e-03 -6.5163253e-03  3.2880188e-03\n",
      " -1.0569858e-03 -6.7875292e-03 -3.2875966e-03 -1.1614132e-03\n",
      " -5.4709408e-03 -1.2113475e-03 -7.5633144e-03  2.6466583e-03\n",
      "  9.0701487e-03 -2.3772502e-03 -9.7651005e-04  3.5135604e-03\n",
      "  8.6650876e-03 -5.9218528e-03 -6.8875789e-03 -2.9329848e-03\n",
      "  9.1476962e-03  8.6626766e-04 -8.6784009e-03 -1.4469790e-03\n",
      "  9.4794659e-03 -7.5494885e-03 -5.3580999e-03  9.3165608e-03\n",
      " -8.9737270e-03  3.8259076e-03  6.6544057e-04  6.6606998e-03\n",
      "  8.3127525e-03 -2.8507852e-03 -3.9923145e-03  8.8979173e-03\n",
      "  2.0896459e-03  6.2489416e-03 -9.4457148e-03  9.5901228e-03\n",
      " -1.3483096e-03 -6.0521150e-03  2.9925345e-03 -4.5661212e-04\n",
      "  4.7064926e-03 -2.2830225e-03 -4.1378425e-03  2.2778988e-03\n",
      "  8.3543826e-03 -4.9956059e-03  2.6686788e-03 -7.9905558e-03\n",
      " -6.7733480e-03 -4.6766998e-04 -8.7677287e-03  2.7894378e-03\n",
      "  1.5985942e-03 -2.3196936e-03  5.0037908e-03  9.7487858e-03\n",
      "  8.4542679e-03 -1.8802262e-03  2.0581507e-03 -4.0036892e-03\n",
      " -8.2414057e-03  6.2779547e-03 -1.9491815e-03 -6.6620589e-04\n",
      " -1.7713332e-03 -4.5356657e-03  4.0617082e-03 -4.2701815e-03]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "#from gensim.models import word2vec\n",
    "\n",
    "\n",
    "sentences = [['this', 'movie', 'is', 'very', 'good'], ['this', 'film', 'is', 'a', 'good'], ['very', 'bad', 'very', 'very', 'bad']]\n",
    "\n",
    "model = Word2Vec(min_count=1, max_vocab_size=10) # 次元数を10に設定\n",
    "model.build_vocab(sentences) # 準備\n",
    "model.train(sentences, total_examples=model.corpus_count, epochs=model.epochs) # 学習\n",
    "\n",
    "#print(\"語彙の一覧 : {}\".format(model.wv.vocab.keys()))\n",
    "print(\"語彙の一覧 : {}\".format(model.wv.key_to_index.keys()))\n",
    "\n",
    "for vocab in model.wv.key_to_index.keys():\n",
    "    print(\"{}のベクトル : \\n{}\".format(vocab, model.wv[vocab]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "id": "awful-fleet",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('this', 0.17018884420394897),\n",
       " ('film', 0.14595063030719757),\n",
       " ('a', 0.0640898048877716)]"
      ]
     },
     "execution_count": 554,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 単語の距離\n",
    "\n",
    "model.wv.most_similar(positive=\"good\", topn=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "id": "structured-letter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'that', 'movie']\n",
      "<class 'list'>\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'KeyedVectors' object has no attribute 'vocabs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-656-ae63b5d4ea0e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mtsne_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTSNE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperplexity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pca\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m23\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#vectors_tsne = tsne_model.fit_transform(model[vocabs])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mvectors_tsne\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtsne_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectors_tsne\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectors_tsne\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'KeyedVectors' object has no attribute 'vocabs'"
     ]
    }
   ],
   "source": [
    "# 可視化\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#vocabs = model.wv.vocab.keys()\n",
    "vocabs = model.wv.key_to_index.keys()\n",
    "print(list(vocabs))\n",
    "print(type(list(vocabs)))\n",
    "tsne_model = TSNE(perplexity=40, n_components=2, init=\"pca\", n_iter=5000, random_state=23)\n",
    "#vectors_tsne = tsne_model.fit_transform(model[vocabs])\n",
    "vectors_tsne = tsne_model.fit_transform(model.wv.vocabs)\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "ax.scatter(vectors_tsne[:, 0], vectors_tsne[:, 1])\n",
    "\n",
    "for i, word in enumerate(list(vocabs)):\n",
    "    plt.annotate(word, xy=(vectors_tsne[i, 0], vectors_tsne[i, 1]))\n",
    "    \n",
    "ax.set_yticklabels([])\n",
    "ax.set_xticklabels([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "posted-robin",
   "metadata": {},
   "source": [
    "## 【問題5】コーパスの前処理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "joined-punishment",
   "metadata": {},
   "source": [
    "#### 小文字化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "id": "improving-karma",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Zero Day leads you to think, even re-think why two boys/young men would do what they did - commit mutual suicide via slaughtering their classmates. It captures what must be beyond a bizarre mode of being for two humans who have decided to withdraw from common civility in order to define their own/mutual world via coupled destruction.<br /><br />It is not a perfect movie but given what money/time the filmmaker and actors had - it is a remarkable product. In terms of explaining the motives and actions of the two young suicide/murderers it is better than 'Elephant' - in terms of being a film that gets under our 'rationalistic' skin it is a far, far better film than almost anything you are likely to see. <br /><br />Flawed but honest with a terrible honesty.\""
      ]
     },
     "execution_count": 581,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "id": "willing-aruba",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_lowed = x_train.copy()\n",
    "\n",
    "for i in range(len(x_train)):\n",
    "    x_train_lowed[i] = x_train[i].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "id": "connected-prefix",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"zero day leads you to think, even re-think why two boys/young men would do what they did - commit mutual suicide via slaughtering their classmates. it captures what must be beyond a bizarre mode of being for two humans who have decided to withdraw from common civility in order to define their own/mutual world via coupled destruction.<br /><br />it is not a perfect movie but given what money/time the filmmaker and actors had - it is a remarkable product. in terms of explaining the motives and actions of the two young suicide/murderers it is better than 'elephant' - in terms of being a film that gets under our 'rationalistic' skin it is a far, far better film than almost anything you are likely to see. <br /><br />flawed but honest with a terrible honesty.\""
      ]
     },
     "execution_count": 587,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_lowed[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dated-storage",
   "metadata": {},
   "source": [
    "#### 特殊文字等の削除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "id": "mechanical-passport",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_preprocessed = x_train_lowed.copy()\n",
    "\n",
    "for i in range(len(x_train)):\n",
    "    x_train_preprocessed[i] = re.sub(r\"[^a-zA-Z0-9]\",\" \",x_train_lowed[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "id": "sticky-wildlife",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'zero day leads you to think  even re think why two boys young men would do what they did   commit mutual suicide via slaughtering their classmates  it captures what must be beyond a bizarre mode of being for two humans who have decided to withdraw from common civility in order to define their own mutual world via coupled destruction  br    br   it is not a perfect movie but given what money time the filmmaker and actors had   it is a remarkable product  in terms of explaining the motives and actions of the two young suicide murderers it is better than  elephant    in terms of being a film that gets under our  rationalistic  skin it is a far  far better film than almost anything you are likely to see   br    br   flawed but honest with a terrible honesty '"
      ]
     },
     "execution_count": 639,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_preprocessed[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vulnerable-arnold",
   "metadata": {},
   "source": [
    "#### リスト化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "id": "antique-ordering",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_list = x_train_preprocessed.copy()\n",
    "\n",
    "for i in range(len(x_train)):\n",
    "    x_train_list[i] = list(x_train_preprocessed[i].split(\" \"))\n",
    "    \n",
    "#list(x_train_preprocessed[0].split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "id": "grand-drain",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['zero',\n",
       " 'day',\n",
       " 'leads',\n",
       " 'you',\n",
       " 'to',\n",
       " 'think',\n",
       " '',\n",
       " 'even',\n",
       " 're',\n",
       " 'think',\n",
       " 'why',\n",
       " 'two',\n",
       " 'boys',\n",
       " 'young',\n",
       " 'men',\n",
       " 'would',\n",
       " 'do',\n",
       " 'what',\n",
       " 'they',\n",
       " 'did',\n",
       " '',\n",
       " '',\n",
       " 'commit',\n",
       " 'mutual',\n",
       " 'suicide',\n",
       " 'via',\n",
       " 'slaughtering',\n",
       " 'their',\n",
       " 'classmates',\n",
       " '',\n",
       " 'it',\n",
       " 'captures',\n",
       " 'what',\n",
       " 'must',\n",
       " 'be',\n",
       " 'beyond',\n",
       " 'a',\n",
       " 'bizarre',\n",
       " 'mode',\n",
       " 'of',\n",
       " 'being',\n",
       " 'for',\n",
       " 'two',\n",
       " 'humans',\n",
       " 'who',\n",
       " 'have',\n",
       " 'decided',\n",
       " 'to',\n",
       " 'withdraw',\n",
       " 'from',\n",
       " 'common',\n",
       " 'civility',\n",
       " 'in',\n",
       " 'order',\n",
       " 'to',\n",
       " 'define',\n",
       " 'their',\n",
       " 'own',\n",
       " 'mutual',\n",
       " 'world',\n",
       " 'via',\n",
       " 'coupled',\n",
       " 'destruction',\n",
       " '',\n",
       " 'br',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'br',\n",
       " '',\n",
       " '',\n",
       " 'it',\n",
       " 'is',\n",
       " 'not',\n",
       " 'a',\n",
       " 'perfect',\n",
       " 'movie',\n",
       " 'but',\n",
       " 'given',\n",
       " 'what',\n",
       " 'money',\n",
       " 'time',\n",
       " 'the',\n",
       " 'filmmaker',\n",
       " 'and',\n",
       " 'actors',\n",
       " 'had',\n",
       " '',\n",
       " '',\n",
       " 'it',\n",
       " 'is',\n",
       " 'a',\n",
       " 'remarkable',\n",
       " 'product',\n",
       " '',\n",
       " 'in',\n",
       " 'terms',\n",
       " 'of',\n",
       " 'explaining',\n",
       " 'the',\n",
       " 'motives',\n",
       " 'and',\n",
       " 'actions',\n",
       " 'of',\n",
       " 'the',\n",
       " 'two',\n",
       " 'young',\n",
       " 'suicide',\n",
       " 'murderers',\n",
       " 'it',\n",
       " 'is',\n",
       " 'better',\n",
       " 'than',\n",
       " '',\n",
       " 'elephant',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'in',\n",
       " 'terms',\n",
       " 'of',\n",
       " 'being',\n",
       " 'a',\n",
       " 'film',\n",
       " 'that',\n",
       " 'gets',\n",
       " 'under',\n",
       " 'our',\n",
       " '',\n",
       " 'rationalistic',\n",
       " '',\n",
       " 'skin',\n",
       " 'it',\n",
       " 'is',\n",
       " 'a',\n",
       " 'far',\n",
       " '',\n",
       " 'far',\n",
       " 'better',\n",
       " 'film',\n",
       " 'than',\n",
       " 'almost',\n",
       " 'anything',\n",
       " 'you',\n",
       " 'are',\n",
       " 'likely',\n",
       " 'to',\n",
       " 'see',\n",
       " '',\n",
       " '',\n",
       " 'br',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'br',\n",
       " '',\n",
       " '',\n",
       " 'flawed',\n",
       " 'but',\n",
       " 'honest',\n",
       " 'with',\n",
       " 'a',\n",
       " 'terrible',\n",
       " 'honesty',\n",
       " '']"
      ]
     },
     "execution_count": 642,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boolean-baking",
   "metadata": {},
   "source": [
    "#### ３文字以下の単語を削除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "id": "opened-screw",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_deled = x_train_list.copy()\n",
    "\n",
    "for i in range(len(x_train)):\n",
    "    x_train_deled[i] = [j for j in x_train_deled[i] if len(j) > 3]\n",
    "    \n",
    "#list(x_train_preprocessed[0].split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "id": "unnecessary-sellers",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['zero',\n",
       " 'leads',\n",
       " 'think',\n",
       " 'even',\n",
       " 'think',\n",
       " 'boys',\n",
       " 'young',\n",
       " 'would',\n",
       " 'what',\n",
       " 'they',\n",
       " 'commit',\n",
       " 'mutual',\n",
       " 'suicide',\n",
       " 'slaughtering',\n",
       " 'their',\n",
       " 'classmates',\n",
       " 'captures',\n",
       " 'what',\n",
       " 'must',\n",
       " 'beyond',\n",
       " 'bizarre',\n",
       " 'mode',\n",
       " 'being',\n",
       " 'humans',\n",
       " 'have',\n",
       " 'decided',\n",
       " 'withdraw',\n",
       " 'from',\n",
       " 'common',\n",
       " 'civility',\n",
       " 'order',\n",
       " 'define',\n",
       " 'their',\n",
       " 'mutual',\n",
       " 'world',\n",
       " 'coupled',\n",
       " 'destruction',\n",
       " 'perfect',\n",
       " 'movie',\n",
       " 'given',\n",
       " 'what',\n",
       " 'money',\n",
       " 'time',\n",
       " 'filmmaker',\n",
       " 'actors',\n",
       " 'remarkable',\n",
       " 'product',\n",
       " 'terms',\n",
       " 'explaining',\n",
       " 'motives',\n",
       " 'actions',\n",
       " 'young',\n",
       " 'suicide',\n",
       " 'murderers',\n",
       " 'better',\n",
       " 'than',\n",
       " 'elephant',\n",
       " 'terms',\n",
       " 'being',\n",
       " 'film',\n",
       " 'that',\n",
       " 'gets',\n",
       " 'under',\n",
       " 'rationalistic',\n",
       " 'skin',\n",
       " 'better',\n",
       " 'film',\n",
       " 'than',\n",
       " 'almost',\n",
       " 'anything',\n",
       " 'likely',\n",
       " 'flawed',\n",
       " 'honest',\n",
       " 'with',\n",
       " 'terrible',\n",
       " 'honesty']"
      ]
     },
     "execution_count": 655,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_deled[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "delayed-input",
   "metadata": {},
   "source": [
    "## 【問題6】Word2Vecの学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "id": "nominated-piece",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55142, 16871410)"
      ]
     },
     "execution_count": 653,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Word2Vec(min_count=1, max_vocab_size=10) # 次元数を10に設定\n",
    "\n",
    "model.build_vocab(x_train_deled) # 準備\n",
    "model.train(x_train_deled, total_examples=model.corpus_count, epochs=model.epochs) # 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "breeding-smart",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
